<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>扩散模型</title>
      <link href="/posts/59918/"/>
      <url>/posts/59918/</url>
      
        <content type="html"><![CDATA[<p>一系列噪声扰动图中的边缘（也就是前向扩散过程），并通过学习将这个过程从噪声转换到数据来生成图(a.k.a,生成扩散过程)。由于排列不变性约束和同时考虑离散局部运动的必要性，有效地采用现有的方法来绘制数据是非常重要的Fs和图的整体拓扑性质。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>无监督异常检测</title>
      <link href="/posts/11901/"/>
      <url>/posts/11901/</url>
      
        <content type="html"><![CDATA[<h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><p><strong>方差</strong>是各个样本数据和<strong>平均数</strong>之差的平方和的平均数</p><p><strong>标准差</strong>（Standard Deviation），又称均方差，是方差的平方根</p><p><strong>均方误差</strong>（Mean Squared Error）是各数据偏离<strong>真实值</strong>差值的平方和 的平均数，均方误差的开方叫均方根误差，均方根误差才和标准差形式上接近。</p><p><strong>方差</strong>是数据序列与均值的关系，而<strong>均方误差</strong>是数据序列与真实值之间的关系</p><p>因此，AE的训练损失函数可以采用简单的MSE</p><p>训练AE并不需要对数据进行标注，所以<strong>AE是一种无监督学习方法</strong></p><p>压缩后得到的隐含特征也可以用来做一些其它工作，比如相似性搜索等</p><p>主成分分析（PCA）的主要目标是将特征维度变小，同时尽量减少信息损失。简而言之，对于一个样本矩阵：</p><ul><li>换特征，找一组新的特征来重新表示；</li><li>减少特征，新特征的数目要远小于原特征的数目。</li></ul><p>通过PCA将n维原始特征映射到维k（k&lt;n）上，称这k维特征为主成分。需要强调的是，不是简单地从n 维特征中去除其余n- k维特征，而是重新构造出全新的k维正交特征，且新生成的k维数据尽可能多地包含原来n维数据的信息。例如，使用PCA将20个相关的特征转化为5个无关的新特征，并且尽可能保留原始数据集的信息。</p><h2 id="生成式对抗网络GAN（Generative-Adversarial-Nets）"><a href="#生成式对抗网络GAN（Generative-Adversarial-Nets）" class="headerlink" title="生成式对抗网络GAN（Generative Adversarial Nets）"></a>生成式对抗网络GAN（Generative Adversarial Nets）</h2><p><a href="https://baijiahao.baidu.com/s?id=1628492430897890432&wfr=spider&for=pc">万字综述之生成对抗网络</a></p><p>VAE与GAN的区别：</p><ol><li><p>GAN 的思路比较粗暴，使用一个判别器去度量分布转换模块（即生成器）生成分布与真实数据分布的距离。</p></li><li><p>VAE 则没有那么直观，VAE 通过约束隐变量 z 服从标准正态分布以及重构数据实现了分布转换映射 X&#x3D;G(z)。</p></li></ol><p><a href="https://zhuanlan.zhihu.com/p/97482962">GAN做异常检测</a></p><p>使用GAN进行异常检测的任务是使用对抗性训练过程建模正常行为，并测量异常评分来检测异常</p><p><a href="https://blog.csdn.net/cloudless_sky/article/details/123697481">https://blog.csdn.net/cloudless_sky/article/details/123697481</a></p>]]></content>
      
      
      <categories>
          
          <category> -文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -unsupervised -VAE -GAD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>general heterophily</title>
      <link href="/posts/10605/"/>
      <url>/posts/10605/</url>
      
        <content type="html"><![CDATA[<h1 id="heterophily"><a href="#heterophily" class="headerlink" title="heterophily"></a>heterophily</h1><p>heterophily与heterogeneity不同。Heterogeneity更多地与推荐系统中的节点类型差异有关，例如用户和项目节点，但heterophily是具有相同类型的节点下的邻居之间的特征或标签差异。传统的GNN通常假设相似的节点（特征&#x2F;类）连接在一起，但“对立吸引”现象也广泛存在于一般图中。</p><hr><p>首先是2022的一篇综述：<font size=4><a href="https://arxiv.org/abs/2202.07082">Graph Neural Networks for Graphs with Heterophily: A Survey</a></font></p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/taxonomy.png" alt="作者在文中使用的分类方法" style="zoom: 50%;" /><h2 id="Non-local-Neighbor-Extension"><a href="#Non-local-Neighbor-Extension" class="headerlink" title="Non-local Neighbor Extension"></a>Non-local Neighbor Extension</h2><p>local neighborhood 的定义在heterophilic图上是不恰当的，因为同一类的节点表现出很高的结构相似性，但彼此之间可能更远。两种方法可以从<strong>遥远</strong>但<strong>信息丰富</strong>的节点中捕获重要的特征，进而改进heterophilic GNNs的能力。</p><h3 id="High-order-neighbor-mixing"><a href="#High-order-neighbor-mixing" class="headerlink" title="High-order neighbor mixing"></a>High-order neighbor mixing</h3><p>从节点的本地一跳邻居和<em>k</em>-hop跳邻居那里接收潜在的表示。代表性工作：</p><ol><li><font color="#FF0000">MixHop</font>：除了one-hop邻居外，还考虑了<strong>two-hop neighbors</strong>来进行消息传播。然后，从不同跳获得的消息通过不同的线性变换进行编码然后mixed by concatenation</li><li><font color="#FF0000">H2GCN</font>：在每个消息传递步骤中聚合来自<strong>higher-order neighbors</strong>的信息。验证了当中心节点的one-hop邻居的标签有条件地独立于该节点的标签时，其two-hop邻居倾向于包含更多的与中心节点同一类的节点。</li></ol><h3 id="Potential-neighbor-discovery"><a href="#Potential-neighbor-discovery" class="headerlink" title="Potential neighbor discovery"></a>Potential neighbor discovery</h3><p>重新考虑异质图中的邻居定义，通过异质性下的整个拓扑探索构建新的structural neighbors。其中，s(v, u) 度量在特定定义的潜在空间中，节点u和v之间的距离。τ是控制邻居数量的阈值。</p><p>$$<br>N_{p}(v)&#x3D;{u:s(v,u)&lt;τ}<br>$$</p><ol><li><font color="#FF0000">Geom-GCN</font>：符合文中所定义的几何关系的节点，也参与了GCN的消息聚合。</li></ol><h2 id="GNN-Architecture-Refinement"><a href="#GNN-Architecture-Refinement" class="headerlink" title="GNN Architecture Refinement"></a>GNN Architecture Refinement</h2><p>一般的GNN结构包含两部分：<br>$$<br>m_{v}^{(l)}&#x3D;AGGREGATE^{(L)} ({h_{u}^{(l-1)}:u\in N(v)}),\<br>h_{v}^{(l)}&#x3D;UPDATE^{(L)}({h_{v}^{(l-1)},m}_{v}^{(l)})<br>$$<br><strong>聚合函数</strong>整合已发现的邻居的信息，<strong>更新函数</strong>将学习到的邻居消息与初始的ego representation结合起来。</p><p>针对异质图上的原始局部邻居和扩展的非局部邻居，现有的GNN体系结构细化方法通过相应地修改聚合和更新函数来充分利用邻居信息。</p><h3 id="Adaptive-Message-Aggregation"><a href="#Adaptive-Message-Aggregation" class="headerlink" title="Adaptive Message Aggregation"></a>Adaptive Message Aggregation</h3><p>在异质图上整合有益信息的关键是区分相似邻居（可能在同一类中）的信息和不相似的邻居（可能属于不同的类别）的信息。</p><p>方法：在聚合操作中，对第l层的节点对(u, v)施加 adaptive edge-aware 的权值$a_{u,v}^{(l)}$来改变聚合操作<br>$$<br>m_{v}^{(l)}&#x3D;AGGREGATE^{(L)} (a_{u,v}^{(l)}{h_{u}^{(l-1)}:u\in N(v)}),\<br>$$<br>文中分别提供了现有方法在谱域和空间域采用的权值分配方案。其中，spectral GNNs使用图信号处理的理论来设计图的滤波器，而spatial GNNs则利用图的结构拓扑来开发聚合策略。</p><h4 id="spectral-domain"><a href="#spectral-domain" class="headerlink" title="spectral domain"></a>spectral domain</h4><p>与同质图上近似计算图傅里叶变换 (graph Fourier transformation) 的拉普拉斯平滑 (Laplacian smoothing)和低通滤波 (low-pass fifiltering)相比，异质图上的spectral  GNNs包括低通和高通滤波器，以自适应地提取低频和高频图信号。其背后的本质直觉在于，低通滤波器主要保留了节点特征的共性(commonality)，而高通滤波器则捕获了节点之间的差异。</p><p><font color="#FF0000"> FAGCN</font>采用self-gating注意力机制，通过将$a_{u,v}^{(l)}$分成两个分量，<em>i.e.</em>,$a_{u,v}^{(l,LP)}$ and  $a_{u,v}^{(l,HP)}$，来学习低频信号和高频信号的比例。通过自适应频率信号学习，FAGCN可以在不同类型的图上实现具有同质性和异质性的表达性能。</p><h4 id="spatial-domain"><a href="#spatial-domain" class="headerlink" title="spatial domain"></a>spatial domain</h4><p>在spatial domain中，异质性gnn需要来自相同或不同类别的邻居的基于拓扑的不同聚合，而不是同质性gnn的average aggregation。因此，应根据空间图的拓扑结构和节点标签来分配邻居的边感知权值(edge-aware weights)。</p><p><font color="#FF0000"> DMP</font>以节点属性为弱标签，并考虑不同消息传递的节点属性异质性，并每条边指定每个属性传播权重。</p><h3 id="Ego-neighbor-separation"><a href="#Ego-neighbor-separation" class="headerlink" title="Ego-neighbor separation"></a>Ego-neighbor separation</h3><p>在异质图上，一个ego node的类标签很可能与其相邻的节点不同。因此，将ego node representation与邻居节点的聚合表示分开encode将有利于可区分的node embedding learning。</p><p>ego-neighbor separation method在聚合过程中分开中心节点的自循环连接，同时把更新过程改为non-mixing operations，例如concatenation，而不是混合操作，例如vanilla GCN中的average。</p><p><font color="#FF0000"> H2GCN</font>首先提出去掉自循环连接(self-loop connection)，并且指出，更新函数中的非混合操作确保了表达性节点的表示能在多轮传播中存活下来，而不会变得非常相似。<font color="#FF0000"> WRGNN</font>对ego-node embedding及其邻居聚合消息使用不同的映射函数。</p><h3 id="Inter-layer-combination"><a href="#Inter-layer-combination" class="headerlink" title="Inter-layer combination"></a>Inter-layer combination</h3><p>前面的两种方法深入研究了GNN的每一层，而Inter-layer combination考虑了层间操作来提高异质性下GNN的表示能力。</p><p>这个方法的直觉是，GNN的浅层(shallow layers)收集局部信息，*e.g.*两层vanilla GCN中的一跳邻居位置，随着层的深入，GNN通过多轮邻居传播逐渐隐式地捕获全局信息。</p><p>在异质性设置下，具有相似信息的邻居，即类标签，可能同时在局部几何(local geometry)和长期全局拓扑中(long-term global topology)定位。因此，结合来自每一层的中间表示有助于利用不同的邻域范围，并同时考虑局部和全局结构属性，从而产生强大的异质性GNNs。</p><p>这一想法最早来自于<font color="#FF0000"> JKNet</font>，它首先在同质图上灵活地捕获不同邻域范围下更好的structure-aware representation。由于异质图上的结构拓扑更加复杂。<font color="#FF0000"> H2GCN</font>将之前所有层的节点表示连接起来，并在最后一层清晰地学习异质性节点特征。</p><h1 id="ICLR2023上的相关论文"><a href="#ICLR2023上的相关论文" class="headerlink" title="ICLR2023上的相关论文"></a>ICLR2023上的相关论文</h1><p><font size=4 > <a href="https://arxiv.org/abs/2211.14065">Beyond Smoothing: Unsupervised Graph Representation Learning with Edge Heterophily Discriminating</a> </font></p><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>已有的方法平滑相邻节点的表示，同时，为给表示学习提供监督信号，他们经常使用保持局部平滑性的目标，即鼓励在同一边内的节点表示、随机游走或子图具有更高的相似性。因此，所有的节点都被迫具有与它们的邻居相似的表示。如下图，所有连接的节点在表示空间中都被推得更近，即使其中一些节点与随机采样的节点相比只是具有适度的特征相似性。</p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/similar representation.png" style="zoom:50%;" /><p><em>(Q1) Is it possible to distinguish between two types of edges in an unsupervised manner ?</em></p><p>在无监督的场景下，很难仅通过节点特征和图结构来区分边缘类型，特别是大量的边连接的是具有中等相似性的节点对。</p><p><em>(Q2) How to effectively couple edge discriminating with representation learning into an integrated UGRL model?</em></p><p>作者想要找到一个良好的相互作用方案，使边识别和表示学习相互促进。</p><h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/framework.png"></p><h3 id="Edge-Discriminating"><a href="#Edge-Discriminating" class="headerlink" title="Edge Discriminating"></a>Edge Discriminating</h3><p>用vectorial structural encoding(a.k.a. positional encoding)对结构特征进行建模，通过将节点结构编码（位置编码）与原始特征连接起来，可以保留有效的证据。（本文使用的结构编码是Dwivedi et al. 2022的基于随机游动扩散过程）</p><p>节点$v_{i}$的$d_{s}$维结构编码$s_{i}$可以通过基于$d_{s}$-step随机游走的graph diffusion来计算</p><p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/rw.png"></p><p>其中$T &#x3D; AD^{-1}$是随机游走的转移矩阵。</p><blockquote><p>图增广方法有特征增广、结构增广、混合增广。</p><p>特征增广主要对图数据中的特征信息进行变换，最常见的手段是节点特征遮盖（NFM），即随机的将图中的一些特征量置为 0；此外，节点特征乱序（NFS）也是一种特征增广方法，其手段为对调不同节点的特征向量。</p><p>结构增广的手段是对图结构信息进行变换，常见的结构增广为边修改（EM），包括对边的增加和删除；另一种结构增广为图弥散（Graph diffusion，GD），其对不同阶的邻接矩阵进行加权求和，从而获取更全局的结构信息。</p><p>混合增广则结合了上述两种增广形式，一个典型的手段为子图采样（SS），即从原图数据中采样子结构成为增广样本。</p><p>——对graph diffusion的一个扩展</p></blockquote><p>以原始特征和结构编码为输入，边鉴别器用两个MLP层估计homophily的概率：</p><p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/mlp.png"></p><p>[·||·]表示连接操作，$h_{i}^{‘}$是节点$v_{i}$的中间嵌入，$\theta_{i,j}$是$e_{i,j}$的估计的homophily概率。为了使估计不受边方向的影响，我们将第二层MLP层应用于不同顺序的嵌入连接(embedding concatenations)。</p><p>利用估计的概率$\theta_{i,j}$，作者想从伯努利分布$w_{i,j}\sim Bernoulli(\theta_{i,j})$中采样一个二分类的homophily指标$w_{i,j}$，$w_{i,j}&#x3D;0$代表homophily，$w_{i,j}&#x3D;1$代表heterophily。然而，这种采样是不可微的，会使得鉴别器难以训练。为解决这个问题，作者采用Gumbel-Max重参数化技巧来近似二进制指标$w_{i,j}$。具体地说，离散的同质性指标$w_{i,j}$ is relaxed to一个连续的homophily weight $\hat{w}_{i,j}$：</p><p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/sigmoid.png"></p><p>其中，$\delta \sim Uniform(0,1)$为抽样的Gumbel随机变量，$\tau_{g}&gt;0$为 temperature hyper-parameter。当$\tau_{g}$接近0时，$\hat{w}_{i,j}$ tends to be sharper（更接近0或1）。</p><p>为了使边鉴别可训练，作者没有明确地将边分为同质和异质两类，而是在后面homophilic&#x2F;heterophilic这两个view上work的时候，为每个边分配一个软权重。</p><p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/softweight.png"></p><p>ranking loss：学习多个输入之间的关系。这种任务也叫metric learning，即通过学习将原始输入投影到一个新的空间中，在这个空间里相似输入将表现出距离相近，而不同输入将表现出距离甚远。</p><h3 id="Dual-Channel-Encoding"><a href="#Dual-Channel-Encoding" class="headerlink" title="Dual-Channel Encoding"></a>Dual-Channel Encoding</h3><p>为了从homophilic和heterophilic view中学习信息表示，作者设计了两个不同的编码器，它们分别在两个不同的vi上执行低通和高通graph filtering。根据homophily边获取相似节点之间共享的信息，根据heterophily边从dissimilar的邻居中过滤出不相关的信息。</p><ol><li>具体地说，在homophilic view中，相似节点彼此连接，我们用低通图滤波器沿着homophilic结构平滑节点特征。低通滤波可以捕获图信号中的低频信息，从而保持相似节点的共享知识。GREET种使用的是一个简单的低通GNN，即SGC。</li><li>与homophilic view不同，heterophilic view包含连接不同节点的边，使用高通图滤波，沿边锐化节点特征并保留高频图信号。从而区分不同但连接的节点的表示。从图信号处理的角度来看，图拉普拉斯算子（即与图拉普拉斯矩阵L相乘）已被证明可以有效地捕获高频分量。因此使用Lap-SGC，来对heterophilic view进行高通滤波。</li></ol><p>使用两个不同的编码器（即SGC和Lap-SGC）获得两组分别捕获低频和高频信息的表示。最终节点表示通过连接由两个视图的表示得到。</p><h3 id="Model-Training"><a href="#Model-Training" class="headerlink" title="Model Training"></a>Model Training</h3><p>使用<strong>Pivot-Anchored Ranking Loss</strong>训练边判别，使用<strong>contrastive loss</strong>训练representation encoders，还引入了一种交替训练策略来迭代优化两个组件。</p><ol><li><strong>Pivot-Anchored Ranking Loss</strong>：边识别的目标是区分homophilic（连接相似节点）和heterophilic（连接不同节点）边，其主要的挑战是找到“相似”和“不同”之间的界限。作者建议使用随机抽样的节点对作为相似度度量的“轴”(Pivot)，以确保由homophilic连接的节点对明显比“轴节点对”更相似，而由heterophilic连接的节点明显比“轴节点对”更不同。</li><li><strong>Robust Dual-Channel Contrastive Loss</strong>：</li><li><strong>Training Strategy</strong> ：边识别模块的训练依赖于节点的表示来度量节点的相似性；表示学习反过来从边识别提供的视图生成表示。为了有效地训练两个组件，随着这两个组件的相互增强，作者采用交替训练策略来交替优化边识别和节点表示学习。总体优化目标可以写为$L&#x3D;L_{r}+L_{c}$。为了提高表示模块的泛化能力，作者采用数据增强来增加模型训练的数据多样性：feature masking和edge dropping，以扰乱两个视图的特征和结构。</li></ol><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><ol><li>作者也用adversarial attack进行攻击，证明模型的鲁棒性</li><li>首尾呼应，依旧可视化了Cora、扰动后的Cora和Texas的 pair-wise representation similarity<ul><li>在(a)中，对于Cora数据集中的大多数边缘，末端节点的表示是相似的；值得注意的是，对于一小部分被识别为heterophilic的边，它们的相似性更接近于0。证明GREET可以分离 homophilic and heterophilic edges，并产生可区分的表示。</li><li>(b)：在受扰动的Cora中，大量的边被检测为heterophilic，两端节点表示是不同的。由于GREET具有识别噪声边缘的能力，因此在对抗攻击面前显示出了很强的鲁棒性</li><li>(c)中也可以发现了类似的现象，其中大部分边被识别为heterophilic。分离的两种类型的边带来了更多的信息表示，从而导致了GREET在heterophilic图上的优越性能。</li></ul></li></ol><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/experiment.png" style="zoom:67%;" />]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAD </tag>
            
            <tag> heterophily </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建站备忘录</title>
      <link href="/posts/56120/"/>
      <url>/posts/56120/</url>
      
        <content type="html"><![CDATA[<p>图标库：<a href="https://fontawesome.com/icons">https://fontawesome.com/icons</a></p><p>新建一篇文章：<code>hexo new post &quot;article title&quot;</code> </p><p>新建页面：<code>hexo new page &quot;page name&quot;</code></p><p>Hexo 默认的静态 <code>url</code> 格式是：<code>:year/:month/:day/:title</code>，是按照年、月、日、文章标题来生成固定链接的。如：<code>http://id.github.io/2022/11/23/hello-world</code>。</p><p>使用 Abbrlink 插件可以使每篇文章都有一个唯一的编号，并将文章的链接用这个编号唯一区别，这样链接中不会出现中文，也不会因为修改文章的日期而导致链接的改变。</p><p>使用gulp压缩静态资源</p><p>直接在markdown中使用和html语法改变文字大小、颜色和背景：</p><p><font size=4 > 这里输入文字，自定义字体大小 </font><br><font color="#FF0000"> 这里输入文字，自定义字体颜色</font><br><span style="background-color: #ff6600;">这里输入文字，自定义字体背景色</span><br><font color="#000000" size=4><span style="background-color: #ADFF2F;">这是综合起来的效果 </span></font><br><font color="#FFFFFF" size=4><span style="background-color: #68228B;">这是综合起来的效果2 </span></font></p>]]></content>
      
      
      <categories>
          
          <category> 建站 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CARE-GNN</title>
      <link href="/posts/45529/"/>
      <url>/posts/45529/</url>
      
        <content type="html"><![CDATA[<h2 id="Enhancing-Graph-Neural-Network-based-Fraud-Detectors-against-Camouflaged-Fraudsters"><a href="#Enhancing-Graph-Neural-Network-based-Fraud-Detectors-against-Camouflaged-Fraudsters" class="headerlink" title="Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters"></a>Enhancing Graph Neural Network-based Fraud Detectors against Camouflaged Fraudsters</h2><blockquote><p>Yingtong Dou1, Zhiwei Liu1, Li Sun2, Yutong Deng2, Hao Peng3, Philip S. Yu1</p></blockquote><p>会议：CIKM ‘20</p><p>原文地址：<a href="https://dl.acm.org/doi/abs/10.1145/3340531.3411903">https://dl.acm.org/doi/abs/10.1145/3340531.3411903</a></p><p>参考翻译：<a href="https://blog.csdn.net/jingcao233/article/details/121718108">https://blog.csdn.net/jingcao233/article/details/121718108</a></p><h2 id="introduction"><a href="#introduction" class="headerlink" title="introduction"></a>introduction</h2><hr><p>Graph-based methods can reveal the suspiciousness of these entities at the graph level, since fraudsters with the same goal tend to connect with each otherTextual features</p><p>GNN-based methods aggregate聚合 neighborhood information to learn the representation of a center node with neural modules. They can be trained in an end-to-end and semi-supervised fashion, which saves much feature engineering and data annotation cost.</p><h2 id="motivation："><a href="#motivation：" class="headerlink" title="motivation："></a>motivation：</h2><ul><li>ignoring the camouflage behaviors of fraudsters</li><li>the limitations and vulnerabilities of GNNs when graphs have noisy nodes and edges</li><li>Though some recent works have noticed similar challenges, either fail to fit the fraud detection problems or break the end-to-end learning fashion of GNNs</li><li>Directly applying GNNs to graphs with camouflaged fraudsters will hamper the neighbor aggregation process of GNNs.</li></ul><p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/1.png" alt="Untitled"></p><h2 id="two-types-of-camouflages"><a href="#two-types-of-camouflages" class="headerlink" title="two types of camouflages:"></a>two types of camouflages:</h2><ul><li>Feature camouflage:  fraudsters may &#x3D;&#x3D;adjust their behaviors&#x3D;&#x3D;, add special characters in reviews, or employ deep language generation models to gloss over explicit suspicious outcomes</li><li>Relation camouflage:  fraudsters camouflage themselves via ‘connecting to many benign entities’</li></ul><h2 id="method"><a href="#method" class="headerlink" title="method:"></a>method:</h2><ol><li>For the feature camouflage: label-aware similarity measure to find the most similar neighbors based on node features. Specifically, we design a neural classifier as a similarity measure, which is directly optimized according to experts with domain knowledge (i.e., annotated data).基于标签感知的相似性度量计算其邻居相似性 </li><li>For the relation camouflage: devise a similarity-aware neighbor selector to select the similar neighbors of a center node within a relation. Furthermore, we leverage reinforcement learning (RL) to adaptively find the optimal neighbor selection threshold along with the GNN training process. </li><li>We utilize the neighbor filtering thresholds learned by RL to formulate a relation-aware neighbor aggregator which combines neighborhood information from different relations and obtains the final center node representation获得中心节点表示</li></ol><h2 id="advantages-amp-benefits"><a href="#advantages-amp-benefits" class="headerlink" title="advantages &amp; benefits:"></a>advantages &amp; benefits:</h2><ul><li>Adaptability. CARE-GNN adaptively selects best neighbors for aggregation given arbitrary multi-relation graph.</li><li>High-efficiency. CARE-GNN has a high computational efficiency without attention and deep reinforcement learning.</li><li>Flexibility. Many other neural modules and external knowledge can be plugged into the CARE-GNN</li></ul><h2 id="steps："><a href="#steps：" class="headerlink" title="steps："></a>steps：</h2><p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/2.png" alt="Untitled"></p><ol><li><p>construct a multi-relation graph based on domain knowledge. </p></li><li><p>first compute its neighbor similarities based with proposed label-aware similarity measure.基于标签感知的相似性度量计算其邻居相似性，通过对比节点特征来寻找最相似的邻居。</p><ol><li>high time complexity×</li><li>employ a one-layer MLP as the node label predictor at each layer and use the l1-distance between the prediction results of two nodes as their similarity measure.在每一层采用一层MLP作为节点标签预测器，并使用两个节点预测结果之间的距离作为它们的相似性度量<ol><li>Optimization. To train the similarity measure together with GNNs, a heuristic approach is to append it as a new layer before the aggregation layer of GCN [ 20 ]. However, if the similarity measure could not effectively filter the camouflaged neighbors at the first layer, it will hamper the performance of following GNN layers. Consequently, the MLP parameters cannot be well-updated through the back-propagation process. To train the similar measure with a direct supervised signal from labels, like [ 35], we define the cross-entropy loss of the MLP at l-layer asv       the similarity measure parameters are directly updated through the above loss function.为了与gnn一起训练相似性度量，一种启发式的方法是在GCN[20]的聚合层之前添加一个新的层。但是，如果相似性度量不能有效地过滤第一层的伪装邻居，则会影响后续GNN层的性能。因此，MLP参数不能通过反向传播过程很好地更新。为了用来自标签的直接监督信号(如[35])训练相似度量，我们定义了l层MLP的交叉熵损失，因为相似度量参数直接通过上述损失函数更新。</li></ol></li></ol></li><li><p>Similarity-aware Neighbor Selector：Then filter the dissimilar neighbors under each relation with the proposed neighbor selector.相似度感知的邻居选择器：使用邻居选择器过滤每个关系下的不同邻居：选择一个中心节点的某个关系下的相似邻居。另外，在训练过程中，使用强化学习（RL）自适应地找到最佳邻居选择阈值</p><ol><li>It selects similar neighbors under each relation using top-p sampling with an adaptive filtering threshold.它使用具有自适应滤波阈值的top-p采样在每个关系下选择相似的邻居。</li><li>formulate the RL process as B(A,f,T).A是action space，f是reward function，T是terminal condation. an initial pr(l)初始阈值<ol><li><p>action: The action represents how RL updates the pr(l) based on the reward</p></li><li><p>reward: The optimal pr(l) is expected to find the most similar (i.e.,minimum distances in Eq.</p></li><li><p>define the reward for epoch e as：epoch e的奖励定义为：</p><ol><li><p>当epoch e ee上新选择的邻居的平均距离小于前一个epoch时，奖励为正，反之为负。但估计累加奖励并不容易，因此设计了无需搜索的贪婪策略，使用即时奖励来更新动作。</p><p> <img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/3.png" alt="Untitled"></p></li></ol></li><li><p>终止条件：</p><ol><li>RL在最近的10个epoch收敛，并发消息最优阈值。 RL模块终止后，过滤阈值固定为最优阈值，直到GNN收敛。</li></ol><p> <img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/4.png" alt="Untitled"></p></li></ol></li></ol></li><li><p>Then, the GNN is trained with partially labeled nodes supervised by binary classification loss functions. The neighbor selector is optimized using reinforcement learning during training the GNN.用二元分类损失函数，使用部分标记节点进行监督训练。训练期间使用强化学习优化邻居选择器：构造一个关系感知的邻居聚合器，该聚合器将来自不同关系的邻居信息结合起来，得到最终的中心节点表示。</p></li><li><p>Instead of directly aggregating the neighbors for all relations, we separate the aggregation part as intra-relation aggregation关系内聚合 and inter-relation aggregation关系间聚合 process. During the intra-relation aggregation process, the embedding of neighbors under each relation is aggregated simultaneously. Then, the embeddings for each relation are combined during the inter-relation aggregation process. </p><ol><li><p>intra-relation neighbor aggregation: </p><p> <img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/5.png" alt="Untitled"></p></li><li><p>inter-relation aggregation: </p><p> <img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/6.png" alt="Untitled"></p></li></ol></li><li><p>Finally, the node embeddings at the last layer are used for prediction.</p></li></ol><p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/7.png" alt="Untitled"></p><p>做出了两个数据集</p><h2 id="存在问题："><a href="#存在问题：" class="headerlink" title="存在问题："></a>存在问题：</h2><p>mlp预测不准的话，会删掉很重要的或者良性的边。</p><p>两种相差很大的benign，互相对对方的学习有帮助</p><p>直接删掉会对模型有很大问题</p><h1 id="Reinforced-Neighborhood-Selection-Guided-Multi-Relational-Graph-Neural-Networks"><a href="#Reinforced-Neighborhood-Selection-Guided-Multi-Relational-Graph-Neural-Networks" class="headerlink" title="Reinforced Neighborhood Selection Guided Multi-Relational Graph Neural Networks"></a>Reinforced Neighborhood Selection Guided Multi-Relational Graph Neural Networks</h1><hr><blockquote><p>HAO PENG∗, Beihang University, China<br>RUITONG ZHANG, Beihang University, China<br>YINGTONG DOU, University of Illinois at Chicago, USA<br>RENYU YANG, University of Leeds, UK<br>JINGYI ZHANG, Beihang University, China<br>PHILIP S. YU, University of Illinois at Chicago, USA</p></blockquote><h1 id="Alleviating-the-Inconsistency-Problem-of-Applying-Graph-Neural-Network-to-Fraud-Detection"><a href="#Alleviating-the-Inconsistency-Problem-of-Applying-Graph-Neural-Network-to-Fraud-Detection" class="headerlink" title="Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection"></a><strong>Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection</strong></h1><hr><blockquote><p>Zhiwei Liu, Yingtong Dou, Yutong Deng, Hao Peng</p></blockquote><p>the inconsistency problem incurred by fraudsters is hardly investigated</p><p><a href="https://www.cnblogs.com/C-W-K/p/13621603.html">《Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud Detection》阅读笔记</a></p><p><a href="https://blog.csdn.net/ypp0229/article/details/108441100">论文阅读笔记：Graphconsis—Alleviating the Inconsistency Problem of Applying Graph Neural Network to Fraud_麦地与诗人的博客-CSDN博客</a></p><h1 id="Pick-and-Choose-A-GNN-based-Imbalanced-Learning-Approach-for-Fraud-Detection"><a href="#Pick-and-Choose-A-GNN-based-Imbalanced-Learning-Approach-for-Fraud-Detection" class="headerlink" title="Pick and Choose: A GNN-based Imbalanced Learning Approach for Fraud Detection"></a>Pick and Choose: A GNN-based Imbalanced Learning Approach for Fraud Detection</h1><hr><p><a href="https://zhuanlan.zhihu.com/p/536942199">中科院敖翔：如何用图神经网络应对互联网金融欺诈？</a></p><p>在sample子图的时候尽可能地选minority点，在构造邻居的时候尽可能增大minority的邻居，减少majority的邻居</p><p>他们专注于修改邻接矩阵来修剪噪声边缘或保持一个平衡的邻域标签频率，</p><h1 id="Rethinking-Graph-Neural-Networks-for-Anomaly-Detection"><a href="#Rethinking-Graph-Neural-Networks-for-Anomaly-Detection" class="headerlink" title="Rethinking Graph Neural Networks for Anomaly Detection"></a>Rethinking Graph Neural Networks for Anomaly Detection</h1><hr><p><a href="https://cloud.tencent.com/developer/article/2021370">ICML 2022 | 基于结构化数据的异常检测再思考: 我们究竟需要怎样的图神经网络？</a></p><p><a href="https://www.pudn.com/news/62d94169864d5c73acdb29d3.html">【GNN报告】香港科技大学李佳：图异常检测再思考-我们究竟需要怎样的图神经网络？-pudn.com</a></p><p><a href="https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/125229976"></a></p><h1 id="FRAUDRE-Fraud-Detection-Dual-Resistant-to-Graph-Inconsistency-and-Imbalance"><a href="#FRAUDRE-Fraud-Detection-Dual-Resistant-to-Graph-Inconsistency-and-Imbalance" class="headerlink" title="FRAUDRE: Fraud Detection Dual-Resistant to Graph Inconsistency and Imbalance"></a>FRAUDRE: Fraud Detection Dual-Resistant to Graph Inconsistency and Imbalance</h1><hr>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAD </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
