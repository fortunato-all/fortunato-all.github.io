<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CARE-GNN</title>
      <link href="/posts/45529/"/>
      <url>/posts/45529/</url>
      
        <content type="html"><![CDATA[<h2id="enhancing-graph-neural-network-based-fraud-detectors-against-camouflaged-fraudsters">EnhancingGraph Neural Network-based Fraud Detectors against CamouflagedFraudsters</h2><blockquote><p>Yingtong Dou1, Zhiwei Liu1, Li Sun2, Yutong Deng2, Hao Peng3, PhilipS. Yu1</p></blockquote><p>会议：CIKM '20</p><p>原文地址：https://dl.acm.org/doi/abs/10.1145/3340531.3411903</p><p>参考翻译：https://blog.csdn.net/jingcao233/article/details/121718108</p><h2 id="introduction">introduction</h2><hr /><p>Graph-based methods can reveal the suspiciousness of these entitiesat the graph level, since fraudsters with the same goal tend to connectwith each otherTextual features</p><p>GNN-based methods aggregate聚合 neighborhood information to learn therepresentation of a center node with neural modules. They can be trainedin an end-to-end and semi-supervised fashion, which saves much featureengineering and data annotation cost.</p><h2 id="motivation">motivation：</h2><ul><li>ignoring the camouflage behaviors of fraudsters</li><li>the limitations and vulnerabilities of GNNs when graphs have noisynodes and edges</li><li>Though some recent works have noticed similar challenges, eitherfail to fit the fraud detection problems or break the end-to-endlearning fashion of GNNs</li><li>Directly applying GNNs to graphs with camouflaged fraudsters willhamper the neighbor aggregation process of GNNs.</li></ul><figure><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/1.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure><h2 id="two-types-of-camouflages">two types of camouflages:</h2><ul><li>Feature camouflage: fraudsters may ==adjust their behaviors==, addspecial characters in reviews, or employ deep language generation modelsto gloss over explicit suspicious outcomes</li><li>Relation camouflage: fraudsters camouflage themselves via‘connecting to many benign entities’</li></ul><h2 id="method">method:</h2><ol type="1"><li>For the feature camouflage: label-aware similarity measure to findthe most similar neighbors based on node features. Specifically, wedesign a neural classifier as a similarity measure, which is directlyoptimized according to experts with domain knowledge (i.e., annotateddata).基于标签感知的相似性度量计算其邻居相似性</li><li>For the relation camouflage: devise a similarity-aware neighborselector to select the similar neighbors of a center node within arelation. Furthermore, we leverage reinforcement learning (RL) toadaptively find the optimal neighbor selection threshold along with theGNN training process.</li><li>We utilize the neighbor filtering thresholds learned by RL toformulate a relation-aware neighbor aggregator which combinesneighborhood information from different relations and obtains the finalcenter node representation获得中心节点表示</li></ol><h2 id="advantages-benefits">advantages &amp; benefits:</h2><ul><li>Adaptability. CARE-GNN adaptively selects best neighbors foraggregation given arbitrary multi-relation graph.</li><li>High-efficiency. CARE-GNN has a high computational efficiencywithout attention and deep reinforcement learning.</li><li>Flexibility. Many other neural modules and external knowledge can beplugged into the CARE-GNN</li></ul><h2 id="steps">steps：</h2><figure><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/2.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure><ol type="1"><li>construct a multi-relation graph based on domain knowledge.</li><li>first compute its neighbor similarities based with proposedlabel-aware similaritymeasure.基于标签感知的相似性度量计算其邻居相似性，通过对比节点特征来寻找最相似的邻居。<ol type="1"><li>high time complexity×</li><li>employ a one-layer MLP as the node label predictor at each layer anduse the l1-distance between the prediction results of two nodes as theirsimilaritymeasure.在每一层采用一层MLP作为节点标签预测器，并使用两个节点预测结果之间的距离作为它们的相似性度量<ol type="1"><li>Optimization. To train the similarity measure together with GNNs, aheuristic approach is to append it as a new layer before the aggregationlayer of GCN [ 20 ]. However, if the similarity measure could noteffectively filter the camouflaged neighbors at the first layer, it willhamper the performance of following GNN layers. Consequently, the MLPparameters cannot be well-updated through the back-propagation process.To train the similar measure with a direct supervised signal fromlabels, like [ 35], we define the cross-entropy loss of the MLP atl-layer asv the similarity measure parameters are directly updatedthrough the above lossfunction.为了与gnn一起训练相似性度量，一种启发式的方法是在GCN[20]的聚合层之前添加一个新的层。但是，如果相似性度量不能有效地过滤第一层的伪装邻居，则会影响后续GNN层的性能。因此，MLP参数不能通过反向传播过程很好地更新。为了用来自标签的直接监督信号(如[35])训练相似度量，我们定义了l层MLP的交叉熵损失，因为相似度量参数直接通过上述损失函数更新。</li></ol></li></ol></li><li>Similarity-aware Neighbor Selector：Then filter the dissimilarneighbors under each relation with the proposed neighborselector.相似度感知的邻居选择器：使用邻居选择器过滤每个关系下的不同邻居：选择一个中心节点的某个关系下的相似邻居。另外，在训练过程中，使用强化学习（RL）自适应地找到最佳邻居选择阈值<ol type="1"><li>It selects similar neighbors under each relation using top-psampling with an adaptive filteringthreshold.它使用具有自适应滤波阈值的top-p采样在每个关系下选择相似的邻居。</li><li>formulate the RL process as B(A,f,T).A是action space，f是rewardfunction，T是terminal condation. an initial pr(l)初始阈值<ol type="1"><li>action: The action represents how RL updates the pr(l) based on thereward</li><li>reward: The optimal pr(l) is expected to find the most similar(i.e.,minimum distances in Eq.</li><li>define the reward for epoch e as：epoch e的奖励定义为：<ol type="1"><li><p>当epoch eee上新选择的邻居的平均距离小于前一个epoch时，奖励为正，反之为负。但估计累加奖励并不容易，因此设计了无需搜索的贪婪策略，使用即时奖励来更新动作。</p><figure><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/3.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure></li></ol></li><li>终止条件：<ol type="1"><li>RL在最近的10个epoch收敛，并发消息最优阈值。RL模块终止后，过滤阈值固定为最优阈值，直到GNN收敛。</li></ol><figure><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/4.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure></li></ol></li></ol></li><li>Then, the GNN is trained with partially labeled nodes supervised bybinary classification loss functions. The neighbor selector is optimizedusing reinforcement learning during training theGNN.用二元分类损失函数，使用部分标记节点进行监督训练。训练期间使用强化学习优化邻居选择器：构造一个关系感知的邻居聚合器，该聚合器将来自不同关系的邻居信息结合起来，得到最终的中心节点表示。</li><li>Instead of directly aggregating the neighbors for all relations, weseparate the aggregation part as intra-relation aggregation关系内聚合and inter-relation aggregation关系间聚合 process. During theintra-relation aggregation process, the embedding of neighbors undereach relation is aggregated simultaneously. Then, the embeddings foreach relation are combined during the inter-relation aggregationprocess.<ol type="1"><li><p>intra-relation neighbor aggregation:</p><figure><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/5.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure></li><li><p>inter-relation aggregation:</p><figure><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/6.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure></li></ol></li><li>Finally, the node embeddings at the last layer are used forprediction.</li></ol><figure><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/7.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure><p>做出了两个数据集</p><h2 id="存在问题">存在问题：</h2><p>mlp预测不准的话，会删掉很重要的或者良性的边。</p><p>两种相差很大的benign，互相对对方的学习有帮助</p><p>直接删掉会对模型有很大问题</p><h1id="reinforced-neighborhood-selection-guided-multi-relational-graph-neural-networks">ReinforcedNeighborhood Selection Guided Multi-Relational Graph NeuralNetworks</h1><hr /><blockquote><p>HAO PENG∗, Beihang University, China<br>RUITONG ZHANG, BeihangUniversity, China<br>YINGTONG DOU, University of Illinois at Chicago,USA<br>RENYU YANG, University of Leeds, UK<br>JINGYI ZHANG, BeihangUniversity, China<br>PHILIP S. YU, University of Illinois at Chicago,USA</p></blockquote><h1id="alleviating-the-inconsistency-problem-of-applying-graph-neural-network-to-fraud-detection"><strong>Alleviatingthe Inconsistency Problem of Applying Graph Neural Network to FraudDetection</strong></h1><hr /><blockquote><p>Zhiwei Liu, Yingtong Dou, Yutong Deng, Hao Peng</p></blockquote><p>the inconsistency problem incurred by fraudsters is hardlyinvestigated</p><p><a href="https://www.cnblogs.com/C-W-K/p/13621603.html">《Alleviatingthe Inconsistency Problem of Applying Graph Neural Network to FraudDetection》阅读笔记</a></p><p><ahref="https://blog.csdn.net/ypp0229/article/details/108441100">论文阅读笔记：Graphconsis---Alleviatingthe Inconsistency Problem of Applying Graph Neural Network toFraud_麦地与诗人的博客-CSDN博客</a></p><h1id="pick-and-choose-a-gnn-based-imbalanced-learning-approach-for-fraud-detection">Pickand Choose: A GNN-based Imbalanced Learning Approach for FraudDetection</h1><hr /><p><ahref="https://zhuanlan.zhihu.com/p/536942199">中科院敖翔：如何用图神经网络应对互联网金融欺诈？</a></p><p>在sample子图的时候尽可能地选minority点，在构造邻居的时候尽可能增大minority的邻居，减少majority的邻居</p><p>他们专注于修改邻接矩阵来修剪噪声边缘或保持一个平衡的邻域标签频率，</p><h1id="rethinking-graph-neural-networks-for-anomaly-detection">RethinkingGraph Neural Networks for Anomaly Detection</h1><hr /><p><a href="https://cloud.tencent.com/developer/article/2021370">ICML2022 | 基于结构化数据的异常检测再思考:我们究竟需要怎样的图神经网络？</a></p><p><ahref="https://www.pudn.com/news/62d94169864d5c73acdb29d3.html">【GNN报告】香港科技大学李佳：图异常检测再思考-我们究竟需要怎样的图神经网络？-pudn.com</a></p><p><ahref="https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/125229976"></a></p><h1id="fraudre-fraud-detection-dual-resistant-to-graph-inconsistency-and-imbalance">FRAUDRE:Fraud Detection Dual-Resistant to Graph Inconsistency and Imbalance</h1>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建站备忘录</title>
      <link href="/posts/56120/"/>
      <url>/posts/56120/</url>
      
        <content type="html"><![CDATA[<p>防止记性不好忘记了或者换电脑找不到啦~~</p><h3 id="网站">网站</h3><p>mac上安装hexo：https://blog.csdn.net/2301_77285173/article/details/130184563另：先安装homebrew在用brew installwget，然后安装nvm（homebrew国内安装方法：https://zhuanlan.zhihu.com/p/111014448/）</p><p>图标库：https://fontawesome.com/icons</p><h3 id="hexo命令">hexo命令</h3><p>新建一篇文章：<code>hexo new post "article title"</code></p><p>新建页面：<code>hexo new page "page name"</code></p><p>Hexo 默认的静态 <code>url</code>格式是：<code>:year/:month/:day/:title</code>，是按照年、月、日、文章标题来生成固定链接的。如：<code>http://id.github.io/2022/11/23/hello-world</code>。</p><p>使用 Abbrlink插件可以使每篇文章都有一个唯一的编号，并将文章的链接用这个编号唯一区别，这样链接中不会出现中文，也不会因为修改文章的日期而导致链接的改变。</p><p>使用gulp压缩静态资源</p><p>【码一下】<span style="background-color: #EDFF3F;">组会完了研究一下<ahref="https://github.com/MoePlayer/hexo-tag-aplayer">音乐</a>和评论</span></p><h3 id="markdown">Markdown</h3><ul><li>直接在markdown中使用和html语法改变文字大小、颜色和背景：</li></ul><p><font size=4 > 这里输入文字，自定义字体大小 </font><font color="#FF0000"> 这里输入文字，自定义字体颜色</font> <spanstyle="background-color: #ff6600;">这里输入文字，自定义字体背景色</span><font color="#000000" size=4><spanstyle="background-color: #ADFF2F;">这是综合起来的效果 </span></font><font color="#FFFFFF" size=4><spanstyle="background-color: #68228B;">这是综合起来的效果2</span></font></p><ul><li><p>正常显示md中的latex公式</p><ul><li><p><code>npm un hexo-math</code><code>npm un hexo-renderer-marked</code></p></li><li><p>安装hexo-renderer-pandoc渲染器：<code>npm i hexo-renderer-pandoc</code></p></li><li><p>官网下载<ahref="https://pandoc.org/installing.html">pandoc</a></p></li><li><p>主题配置下的mathjax设置：</p><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li><p>（在有latex公式的文档前面加<code>mathjax: true</code>）</p></li><li><p>配置到这儿就可以正常显示了，后面还有问题的可以参考这篇文章：https://blog.csdn.net/qq_52466006/article/details/126924064</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> tips </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 建站 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CDSI</title>
      <link href="/posts/38824/"/>
      <url>/posts/38824/</url>
      
        <content type="html"><![CDATA[<p>. Unlike existing score-based diffusion models, the reverse processcan take observations (on the top left of</p><p>the fifigure) as a conditional input, allowing the model to exploitinformation in the observations for</p><p>denoising. We utilize an attention mechanism to capture the temporaland feature dependencies of</p><p>time series</p><p>本文利用一种注意机制来捕捉时间序列的时间依赖性和特征依赖性</p><p>在实践中，我们并不知道ground truth missing value（i.e., imputationtargets），或训练数据可能根本不包含缺失值。然后，受masked languagemodeling的启发，本文开发了一种自监督（self-supervised）的训练方法，将观察值分离成条件信息和计算目标</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> diffusion model </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>扩散模型——分子图生成</title>
      <link href="/posts/42283/"/>
      <url>/posts/42283/</url>
      
        <content type="html"><![CDATA[<h1id="综述-generative-diffusion-models-on-graphs-methods-and-applications">综述-GenerativeDiffusion Models on Graphs: Methods and Applications</h1><ol type="1"><li>主要应用在molecule and protein modeling</li><li>现有方法分为两类：<ul><li>自回归生成（autoregressivegeneration）：按照决策序列的顺序一步一步地生成期望的图（具有拓扑结构和整个图节点/边缘特征）。<ul><li>单样本生成（one-shotgeneration）：在一个步骤中生成具有拓扑结构和节点/边特征的完整的图</li></ul></li></ul></li><li>分子图生成面临的挑战：<ul><li>Discreteness（离散性）：图结构是离散的，导致模型的梯度计算困难。因此不能将广泛使用的优化算法直接以端到端的方式引入到图生成的反向传播训练中。</li><li>Complex IntrinsicDependencies（复杂的内在依赖）：与图像数据不同，节点不是独立同分布（i.i.d.）的，这种图结构的复杂性为生成所需的图带来了巨大的挑战。</li><li>PermutationInvariant（置换不变性）：由于节点在大多数图中自然是无序的，所以最多有<em>N!</em>个不同的等效邻接矩阵代表具有N个节点的相同图。</li></ul></li></ol><h2 id="related-work">Related work</h2><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/diffusion/20230521192801.png" style="zoom:65%;" /></p><ol type="1"><li>GraphVAE构造两个GNN作为encoder和decoder来估计图的分布，但由于基于似然的方法，VAE难以通过后验估计来生成真实的大规模图，并且需要昂贵的计算来实现排列不变性。</li><li>基于GAN的方法更容易出现图结构数据的模式崩溃，并且需要额外的计算来训练discriminator。由于图的离散性，大多数基于GAN的方法都通过强化学习技术进行优化</li><li>归一化流利用一系列可逆函数f(x)来映射图样本，但由于特殊架构的约束，基于流的生成模型（flow-basedgenerative models）很难完全学习图形的结构信息。</li><li>受非平衡热力学理论的启发，扩散生成范式可以被建模为用变分推理训练的马尔可夫链</li></ol><h2 id="diffusion-models">Diffusion Models</h2><p>扩散模型的三种范式：SMLD和DDPM分别利用分数匹配思想和非平衡热力学来学习扩散过程的不同反向函数。SGM推广了离散扩散的计算方法并进一步利用随机微分方程（SDE）对扩散过程进行建模。</p><ol type="1"><li>Score Matching with Langevin Dynamics (SMLD) <font color="#FF0000">score matching是什么意思</font><ol type="1"><li>逐步向数据分布添加随机噪声直到预定义的先验（通常是高斯噪声），然后通过学习数据分布的梯度<spanclass="math inline">\(\nabla_{\mathbf{x}} \logp(\mathbf{x})\)</span>来逆转扩散过程，SMLD用一系列可以建模的增量噪声<spanclass="math inline">\(q_{\sigma}(\tilde{\mathbf{x}} \mid\mathbf{x}):=\mathcal{N}\left(\tilde{\mathbf{x}} \mid \mathbf{x},\sigma^{2} I\right)\)</span>来扰动原始分布</li></ol></li><li>Denoising Diffusion Probabilistic Model (DDPM)<ol type="1"><li></li></ol></li><li>Score-based Generative Model (SGM)<ol type="1"><li>w</li></ol></li></ol><h2 id="future-challenges-and-opportunities">Future Challenges andOpportunities</h2><ol type="1"><li>Discrete Nature ofGraphs：现有的图像扩散模型都是在连续空间中开发的。相比之下，图结构数据的离散特性使得很难直接部署扩散模型。一些工作试图通过引入离散概率分布或弥合连续和连续数据之间的差距，使扩散模型适合用于离散数据离散空间，但仍然缺乏一种普遍的和公认的方法来解决这个问题。</li><li>Conditional Generation for Graph DiffusionModels：将条件纳入生成模型对于指导所需的生成至关重要，而不是生成新的随机样本，生成具有特定性质的分子和蛋白质具有重要的意义。因此，将额外的信息作为条件引入图扩散模型已成为一个必要的研究方向。一种类型的额外知识可以通过知识图谱形成。在特定领域使用知识图可以帮助控制生成过程以获得所需的图，并增强graph生成的多样性。除了知识图外，还可以考虑其他辅助知识（如视觉知识和文本知识）来推进图扩散模型的设计。</li><li>Trustworthiness for Graph DiffusionModels：近年来，人们对人工智能模型可信度的关注日益增加。作为最具代表性的人工智能应用程序之一，graph生成可能会对不同现实任务的用户造成意外伤害，特别是在药物发现等安全关键领域的任务。例如，数据驱动的graph图扩散模型是脆弱的，很容易受到来自恶意攻击者的对抗性攻击；由于图扩散架构的复杂性，理解和解释图生成的工作机制是非常具有挑战性的。在实现可信的图生成方面有几个关键的维度，如安全性和鲁棒性、可解释性、公平性和隐私。因此，如何建立可信的图扩散模型已成为学术界和工业界的关键。</li><li>EvaluationMetrics：对图生成的评估仍然是一个挑战。大多数现有的度量标准通常是基于图的统计量和属性（例如，节点的度和稀疏性），这不是完全可靠的。同时，图生成的有效性和多样性在不同的应用中也很重要。因此，需要努力定量地测量生成的图的质量。</li></ol><h2 id="未来前景">未来前景</h2><ol type="1"><li>RecommenderSystems：图上的扩散模型有可能对给定用户的项目进行条件分布建模，以便更好地为用户生成推荐列表。</li><li>Graph AnomalyDetection：扩散模型可以用来净化图像数据，以获得更好的对抗性鲁棒性。因此，图的扩散模型为改进图的异常检测提供了巨大的机会，从而使增强了图模型对对抗性攻击的鲁棒性。</li><li>Causal GraphGeneration：因果推理是指旨在建立因果关系之间联系的统计数据，通常由因果关系图形成。实际上，分析了因干扰而引起的因果关系是很困难的。例如，临床试验不是简单地使用控制变量，而是使用因果推理来评估该治疗方法的有效性。在因果发现任务中，可以生成因果-效应图，以帮助分析因果关系之间的联系，从而提高下游任务的准确性和获得可解释性。因此，图扩散模型提供了增强因果效应图生成的机会，这有助于减少可能的偏差，建立健壮的模型，并带来解释模型如何工作的新见解。</li></ol><h2id="conditional-diffusion-based-on-discrete-graph-structures-for-molecular-graph-generation">ConditionalDiffusion Based on Discrete Graph Structures for Molecular GraphGeneration</h2><h3 id="motivation">Motivation</h3><p>学习分子图的潜在分布和生成高保真样本是药物发现和材料科学中的一个基础研究问题。然而，准确地建模分布和快速生成新的分子图仍然是关键和具有挑战性的目标。</p><p>现有的主要问题是分子图的质量和采样速度，作者认为，其背后有两个原因：</p><ol type="1"><li>专注于实数图的表述(将分子表示为节点特征和边特征矩阵)，而忽略了离散的图结构，使得难以从有噪声的实数矩阵中提取准确的局部基元进行去噪并保持接近真实的图分布</li><li>一个简单的图神经网络设计可能不足以完全从损坏的图中建模节点边依赖关系，并进一步满足复杂的生成需求</li></ol><p>考虑图的离散性和设计合适的图噪声预测模型</p><h4 id="methodology">Methodology</h4><h5 id="conditional-graph-diffusion">Conditional Graph Diffusion</h5><p>首先定义一个正向过程，该过程用噪声序列扰动数据，直到输出分布成为已知的先验分布。</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/diffusion/20230514201607.png" style="zoom:70%;" /></p><p>将graph构造为由节点特征矩阵和边类型矩阵组成的高维变量即可应用扩散模型<span class="math inline">\(G \in R^{N\times F} \times R^{N\timesN}\)</span>，作者认为，那些被忽略的离散图结构（例如星和环）可能为节点边依赖建模和图去噪提供额外的线索。因此提出，将边存在矩阵（邻接矩阵）和边类型矩阵分离，并且使用one-bit的离散变量来表示该边是否存在，从而形成了<spanclass="math inline">\(\bar{A} \in \left \{0,1\right \}^{N\timesN}\)</span>，进而在每个time step，<span class="math inline">\(t\in[0,T]\)</span>量化离散图结构<spanclass="math inline">\(\bar{A}_t\)</span>。离散图结构可以插入反向过程，并作为条件作用。</p><p>score function是求最大对数似然函数中让对数似然函数梯度等于0的梯度</p><h5 id="graph-noise-prediction-model">Graph Noise Prediction Model</h5><p>由于<spanclass="math inline">\(\epsilon_\theta(G_t,\bar{A}_t,t)\)</span>能够预测添加到原始图像的噪声，作者将其看作图噪声预测模型。由于实时的实数图状态和图分布学习的复杂要求，直接应用图神经网络是不合适的。并且就分子图而言，这一模型应该关注化学价规则的局部节点边依赖关系并且尝试恢复全局图模式（例如边稀疏性、频繁的环子图，甚至是原子型分布）。</p><p>为满足上述挑战，作者提出的混合消息传递模块（hybrid message passingblock，HMPB）包含两个不同种类的消息传递层来显式地建模实数矩阵（<spanclass="math inline">\(X_t\)</span>和<spanclass="math inline">\(A_t\)</span>）和离散矩阵（<spanclass="math inline">\(\bar{A}_t\)</span>）中的结构和特征依赖。一种是标准的消息传递层（例如GINE），依赖于解码的离散图结构来聚合局部邻居节点边特征；另一种是一个全连接的基于注意力的消息传递层，专注于全局信息的提取和传递。将第1个HMPB中的节点和边表示更新过程表示为：</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/diffusion/20230514211209.png" style="zoom:67%;" /></p>]]></content>
      
      
      
        <tags>
            
            <tag> diffusion model </tag>
            
            <tag> 分子图生成 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图神经网络</title>
      <link href="/posts/47450/"/>
      <url>/posts/47450/</url>
      
        <content type="html"><![CDATA[<p>综述：A Comprehensive Survey on Graph Neural Networks</p><p>https://cloud.tencent.com/developer/article/1902489</p><p>https://www.zhihu.com/question/54504471/answer/332657604</p><p>https://zhuanlan.zhihu.com/p/538914712</p><p>https://blog.csdn.net/qq_44689178/article/details/123396737</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Graph Transformers</title>
      <link href="/posts/46224/"/>
      <url>/posts/46224/</url>
      
        <content type="html"><![CDATA[<h1 id="一些关于transformer">一些关于transformer</h1><ol type="1"><li><p>cls：classification，用于下游的分类任务。CLS就是一个向量，只是不是某一个字的向量，是一个够代表整个文本的的语义特征向量，取出来就可以直接用于分类了。实际场景：问答（判断一个问题与一个答案是否匹配）、语句匹配（两句话是否表达同一个意思）等</p></li><li><p>BERT模型除了添加[CLS]符号并将对应的输出作为文本的语义表示，还对输入的两句话用一个[SEP]符号作分割，并分别对两句话附加两个不同的文本向量以作区分，如下图所示。</p></li><li><p>RNN的两个明显问题：<img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/rnn.jpg" style="zoom:50%;" /></p><ol type="1"><li><p>效率问题：需要逐个词进行处理，后一个词要等到前一个词的隐状态输出以后才能开始处理</p></li><li><p>如果传递距离过长还会有梯度消失、梯度爆炸和遗忘问题</p><p>LSTM、GRU的改进属于换汤不换药。</p></li></ol><p>为解决这些问题，设计的Transformer是一个N进N出的结构，每个Transformer单元相当于一层的RNN层，接收一整个句子所有词作为输入，然后为句子中的每个词都做出一个输出。但是与RNN不同的是，Transformer能够同时处理句子中的所有词，并且任意两个词之间的操作距离都是1，很好地解决了上面提到的RNN的效率问题和距离问题。</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/transformer.jpg" style="zoom:50%;" /></p><p>decoder比encoder多了一共attention，用于接受encoder的输出：</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/20230331172600.png" style="zoom: 67%;" /></p><p>详细结构：</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/20230331172734.png" style="zoom:70%;" /></p><ul><li><p>encoder的输入包含两个，是一个序列的token embedding + positionalembedding</p></li><li><p>self-attention层：句子中的某个词对于本身的所有词做一次Attention，算出每个词对于这个词的权重，然后将这个词表示为所有词的加权和。</p><p>首先，每个词都要通过三个矩阵<span class="math inline">\(W_q\)</span>,<span class="math inline">\(W_k\)</span>, <spanclass="math inline">\(W_v\)</span>进行一次线性变化，一分为三，生成每个词自己的query,key, vector三个向量。以一个词为中心进行SelfAttention时，都是用这个词的key向量与每个词的query向量做点积，再通过Softmax归一化出权重。通过这些权重算出所有词的vector的加权和，作为这个词的输出。（通过query 和 key 的相似性程度来确定 value 的权重分布的方法被称为scaleddot-product attention。）</p><p>self-attention的特点在于无视词之间的距离直接计算依赖关系，能够学习一个句子的内部结构，实现也较为简单并且可以并行计算。从一些论文中看到，self-attention可以当成一个层和RNN,CNN,FNN等配合使用，成功应用于其他NLP任务。</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/20230331173129.png" style="zoom:50%;" /></p><p>归一化之前需要通过除以向量的维度dk来进行标准化，所以最终SelfAttention用矩阵变换的方式可以表示为: <span class="math display">\[Q=XW_Q\\K=XW_K\\V=XW_V\\Attention(Q,K,V)=Softmax(\frac{QK^T}{\sqrt{d_k}})V\]</span> 上文提到Encoder中的SelfAttention与Decoder中的有所不同，Encoder中的Q、K、V全部来自于上一层单元的输出，而Decoder只有Q来自于上一个Decoder单元的输出，K与V都来自于Encoder最后一层的输出。也就是说，Decoder是要通过当前状态与Encoder的输出算出权重后，将Encoder的编码加权得到下一层的状态。</p></li><li><p>Multi-head-attention：Multi-HeadAttention就是将上述的Attention做h遍，然后将h个输出进行concat得到最终的输出。这样做可以很好地提高算法的稳定性，在很多Attention相关的工作中都有相关的应用。Transformer的实现中，为了提高Multi-Head的效率，将W扩大了h倍，然后通过view(reshape)和transpose操作将相同词的不同head的k、q、v排列在一起进行同时计算，完成计算后再次通过reshape和transpose完成拼接，相当于对于所有的head进行了一个并行处理。</p></li><li><p>Masked-attention：Encoder要编码整个句子，所以每个词都要考虑上下文的关系。所以每个词在计算的过程中都是可以看到句子中所有的词的。但是Decoder与Seq2Seq中的解码器类似，每个词都只能看到前面词的状态，所以是一个单向的Self-Attention结构。MaskedAttention的实现也非常简单，只要在普通的SelfAttention的Softmax步骤之前，与(&amp;)上一个下三角矩阵M就好了 <spanclass="math display">\[Attention(Q,K,V)=Softmax(\frac{QK^T \odot M}{\sqrt{d_k}})V\]</span></p></li><li><p>Position-wise Feed ForwardNetworks：Encoder中和Decoder中经过Attention之后输出的n个向量（这里n是词的个数）都分别的输入到一个全连接层中，完成一个逐个位置的前馈网络。</p><ul><li>FFN是两层全连接。这里使用FFN层的原因是：为了使用非线性函数来拟合数据。如果说只是为了非线性拟合的话，其实只用到第一层就可以了，但是这里为什么要用两层全连接呢，是因为第一层的全连接层计算后，其维度是(batch_size,seq_len,dff)（其中dff是超参数的一种，设置为2048），而使用第二层全连接层是为了进行维度变换，将dff转换为初始的d_model(512)维。</li></ul><p><span class="math display">\[FFN(x)=max(0,xW_1+b_1)W_2+b_2\]</span></p></li><li><p>Add &amp;Norm：是一个残差网络，将一层的输入与其标准化后的输出进行相加即可。Transformer中每一个SelfAttention层与FFN层后面都会连一个Add &amp;Norm层。该层是为了对attention层的输出进行分布归一化，转换成均值为0方差为1的正态分布。cv中经常会用的是batchNorm，是对一个batchsize中的样本进行一次归一化，而layernorm则是对一层进行一次归一化，二者的作用是一样的，只是针对的维度不同，一般来说batchnorm的输入维度是(batch_size,seq_len,embedding)，针对的是batch_size层进行处理，而layernorm则是对seq_len进行处理（即batchnorm是对一批样本中进行归一化，而layernorm是对每一个样本进行一次归一化）。使用ln而不是bn的原因是因为输入序列的长度问题，每一个序列的长度不同，虽然会经过padding处理，但是padding的0值其实是无用信息，实际上有用的信息还是序列信息，而不同序列的长度不同，所以这里不能使用bn一概而论。</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/20230331175224.png" style="zoom:50%;" /></p></li><li><p>PositionalEncoding：Transformer中句子里的所有词都被同等的看待，所以词之间就没有了先后关系，很可能会带上和词袋模型相同的不足。因此，需要给每个输入的词向量叠加一个固定的向量来表示它的位置<span class="math display">\[PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\\PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\]</span>pos是词在句子中的位置，i是词向量中第i位，即将每个词的词向量为一行进行叠加，然后针对每一列都叠加上一个相位不同或波长逐渐增大的波，以此来唯一区分位置。</p></li></ul></li><li><p>GPT(GenerativePre-Training)，是OpenAI在2018年提出的模型，利用Transformer模型来解决各种自然语言问题，例如分类、推理、问答、相似度等应用的模型。GPT采用了Pre-training+Fine-tuning的训练模式，使得大量无标记的数据得以利用，大大提高了这些问题的效果。</p><ul><li>Decoder Block中使用的是MaskedSelf-Attention，即句子中的每个词，都只能对包括自己在内的前面所有词进行Attention，这就是单向Transformer。GPT使用的Transformer结构就是将Encoder中的Self-Attention替换成了MaskedSelf-Attention</li></ul></li><li><p>关系归纳偏置（relational inductive biases）</p></li><li><p>讲解一些编码：https://blog.csdn.net/zjc910997316/article/details/121524624</p></li><li><p>https://blog.csdn.net/qq_38253797/article/details/127620115</p></li><li><p>https://mp.weixin.qq.com/s?__biz=MzI4MDYzNzg4Mw==&amp;mid=2247493785&amp;idx=3&amp;sn=5730ed0d277e049f31880a0ceba7629f&amp;chksm=ebb7d04ddcc0595bd9b38659832f8c8b681d374fe29108337e491a3c6d60b3a4fa873939294b&amp;scene=27</p></li></ol><h1id="graph-formers-gnn-nested-transformers-for-representation-learning-on-textual-graph">Graph-Formers:GNN-nested Transformers for Representation Learning on TextualGraph</h1><p><em>from NIPS 2021, <ahref="https://github.com/microsoft/GraphFormers">code</a></em></p><h3 id="motivation">Motivation：</h3><p>文本图上的表示学习是基于<strong>单个的文本特征</strong>和<strong>邻域信息</strong>为节点生成低维embeddings。现有的方法主要依赖于cascadedmodel architecture：节点的texturalfeatures首先由语言模型独立编码；texturalembeddings再通过GNN进行聚合。但是由于对texturalfeatures的独立建模，上述体系结构受到了限制。</p><p>本文提出的GraphFormers中，GNN组件按层嵌套在the transformer blocks oflanguage models旁边，使用该框架后，<strong>textencoding</strong>和<strong>graph aggregation</strong> are fused into anditerative workflow，从全局的角度准确地理解每个节点的语义(semantic)。</p><p>另外，还是用了一个progressive learningstrategy，通过对操作数据(manipulated data)和原始数据(originaldata)进行连续(successively)训练，增强模型对图上信息的整合能力。</p><h3 id="intro">Intro：</h3><p>目前的主流方法是Pre-trained LanguageRepresentation(PLM，例如BERT，获得文本的底层语义)结合GNN(聚合邻域信息，获得moreinformative的embeddings)。这种组合方式叫做CascadedTransformers-GNN，因为transformers部署在GNN前面，<font color="#FF0000"></font> 。但是，考虑到linkednodes是相互关联的，<strong>在生成embedding时，其底层语义可以相互增强</strong>。例如，给定一个节点“noteson transformers”和他的邻居的“tutorials on machinetranslation”，通过参考整个上下文，这里的“transformers”可以被解释为一个机器学习模型，而不是一个电子设备变压器。</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/20230311204736.png" alt="Cascaded Transformers-GNN：text embeddings are independently generated by language models and aggregated by rear-mounted GNNs"  /></p><p>在本文中，文本编码(transformererlayers)和图聚合(GNN)迭代进行，在每个迭代后，linkednodes在layerwise的GNN组件中相互交换信息；因此，<font color="#FF0000">每个节点都将被其邻域信息增强（在这里可以使用异质性？）</font>。然后transformer在增强的节点特征上work，这个节点特征可以为下一次迭代生成信息更丰富的节点表示。与级联架构相比，GraphFormers图上的cross-nodeinformation进行了更充分的利用，大大提高了表示质量。考虑到分层的GNN组件只涉及简单和有效的<font color="#FF0000">multi-headattention</font>，GraphFormers保持了与现有的级联模型相当的运行成本。</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/Snipaste_2023-03-11_20-52-33.png" alt="GNN-nested Transformers: the text encoding and graph aggregation are iteratively performed with the layerwise GNNs and Transformers(TRM)." style="zoom:90%;" /></p><p>训练过程中：</p><h3 id="method">Method</h3><p>每个节点x都是一个text，节点x及其相邻节点<spanclass="math inline">\(N_x\)</span>记为<spanclass="math inline">\(G_x\)</span>。模型基于节点x的文本特征和他的邻域信息来学习嵌入。生成的嵌入被期望捕获节点之间的关系，<font color="#FF0000">即基于嵌入的相似度准确地预测两个节点是否是而进行连接的。</font></p><p>关于<strong>token embedding</strong>，</p><h4 id="model-simplification">model simplification</h4><p>训练任务是链路预测</p><h1id="graphtrans-representing-long-range-context-for-graph-neural-networks-with-global-attention">GraphTrans:Representing Long-Range Context for Graph Neural Networks with GlobalAttention</h1><p>graph classification task（text-classification）</p><h3 id="cls">CLS：</h3><p>将图输入GNN获得图中每个节点的向量表示（让节点的向量表示获取位置信息），之后将每个节点的向量表示输入标准的Transformer（将自然语言处理中的CLS作为一种readout机制引入图分类）</p><p>在节点序列的首位置加入一个CLS节点（因为使用的Transformer不带positionalencoding，所以CLS的位置可以随意选取），之后使用经过Transformer的CLS表示进行图分类任务。</p><p>实质上，CLS readout可以做是virtual node的generalization或者deepversion.（This special-token readout mechanism may be viewed as ageneralization or a “deep” version of a virtual node readout.但是virtualnode method不允许学习图节点之间的成对关系，除了在虚拟节点的嵌入内）</p><h1 id="graphgps-recipe-for-a-general-powerful-scalable">GraphGPS:Recipe for a General, Powerful, Scalable</h1><h3id="对现有的pepositional-emcodings和sestructural-encodings进行分类">对现有的PE(positionalemcodings)和SE(structural encodings)进行分类</h3><p>各自作用：PE gives a notion of <strong>distance</strong>, while SEgives a notion of <strong>structural similarity</strong>.</p><p>One can always infer certain notions of distance from largestructures, or certain notions of structure from short distances, butthis is not a trivial task, and the objective of providing PE and SEremains distinct.</p><p>PE是为了提供图中给定节点在空间中的位置的概念。因此，当在图或子图中两个节点彼此接近时，它们的PE也应该接近。一种常见的方法是计算每对节点或其特征向量之间的pair-wisedistance，但这与Transformer器不兼容，因为它需要实现full attentionmatrix。相反，我们希望PE是节点或者边的特征，因此，一个更好的拟合解是使用图的拉普拉斯矩阵的特征向量或它们的梯度。</p><p>SE旨在提供图或子图结构的embedding，以提高GNN的expressivity and thegeneralizability。因此，当两个节点共享相似的子图时，或者当两个图相似时，它们的SE也应该很接近。简单的方法是将图中的pre-definedpatterns识别为一个热编码，但它们需要对图的专家知识。</p><p>这些编码如何增加MPNNs的表达</p><p>参考链接：</p><p>https://blog.csdn.net/yeen123/article/details/125104680</p><p>https://zhuanlan.zhihu.com/p/604450283</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态图异常检测——HOG-GCN</title>
      <link href="/posts/52924/"/>
      <url>/posts/52924/</url>
      
        <content type="html"><![CDATA[<p>这篇文章是2022一篇综述里的，看之前没有意识到是前几年在spatialdomain的研究，但是看了还是写下来了。</p><h2id="powerful-graph-convolutional-networks-with-adaptive-propagation-mechanism-for-homophily-and-heterophilyhog-gcn">PowerfulGraph Convolutional Networks with Adaptive Propagation Mechanism forHomophily and Heterophily(HOG-GCN)</h2><p>Motivation：let the propagation mechanism of GCN essentially suitablefor both homophily andheterophily，根据节点对之间的同质性进行聚合。首先从attribute和topologyspace中学习homophily degreematrix，并进一步用它在邻域之间进行特征传播；在传播过程可以通过下游半监督任务帮助学习更好的homophilydegree matrix</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/20230306105851.png" style="zoom: 50%;" /></p><h3 id="homophily-degree-matrix-estimation">Homophily Degree MatrixEstimation</h3><p><strong>attribute space</strong>：graph-agnostic MLP</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/20230310182653.png" style="zoom: 50%;" /></p><p>其中，<span class="math inline">\(B_{ic}\)</span>表示表示节点<spanclass="math inline">\(v_i\)</span>属于<em>class c</em>的概率，<spanclass="math inline">\(S_{ij}=b_i b_j^T\)</span>表示节点<spanclass="math inline">\(v_i\)</span>和节点<spanclass="math inline">\(v_j\)</span>属于同一类的程度。</p><p><strong>topology space：</strong>label propagation</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/20230310183426.png" style="zoom: 33%;" /></p><p>标签传播算法：每一次迭代都会根据与自己相连的节点所属的标签改变自己的标签，更改的原则是选择与其相连的节点中所属标签最多的社区标签为自己的社区标签。随着社区标签不断传播。最终，连接紧密的节点将有共同的标签。经典的标签传播通常假设两个连接的节点更有可能具有相同的类，从而在邻域之间迭代地传播标签。然而，经典的标签传播技术旨在捕捉同质性的假设，这不能直接适应具有同质性的网络。</p><p>关键的直觉是，类内标签之间的影响大于类间标签之间的影响。因此，学习到的权重矩阵可以用来表示两个节点所达到的程度属于同一个类。由于网络表现出不同程度的异质性，我们在网络的k阶结构上进行标签传播，以捕获更多的亲同性节点（例如，k= 2）。</p><p>值得注意的是，同质度矩阵S是基于原始节点属性估计的，这些属性不受网络异质性的约束。因此，它也适用于具有异质性或低同质性的网络。</p><p>根据得到的S和T，我们可以得到homophily matrix：<spanclass="math inline">\(h=\alpha S+\beta T\)</span></p><h3 id="homophily-guided-propagation">Homophily-guided Propagation</h3><p>在spatial domain为不同的邻居分配不同的权重。</p><p><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/20230310184157.png" /></p><p>实验验证，k=2时，效果最好。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> heterophily </tag>
            
            <tag> spatial domain </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>动态图异常检测数据集，及GearSage代码详解</title>
      <link href="/posts/62968/"/>
      <url>/posts/62968/</url>
      
        <content type="html"><![CDATA[<h2 id="传统意义的时序异常检测"><ahref="https://www.zhihu.com/people/muzhen-14/posts?page=1">传统意义的时序异常检测</a></h2><p>随着车辆、工业系统和数据中心等网络物理系统（CPS）中的互联设备和传感器的快速增长，监测这些设备的需求越来越大以保护他们免受攻击。对于电网、水处理厂、交通和通信网络等关键基础设施尤其如此。</p><p>许多这样的现实世界系统涉及到大量的相互连接的传感器，这些传感器可以产生大量的时间序列数据。例如，在一个水处理厂中，可以有许多传感器测量其各部件中的水位、流量、水质、阀门状态等。<strong>来自这些传感器的数据可以以复杂的、非线性的方式联系起来</strong>：例如，一个阀门导致压力和流量的变化，导致进一步的变化，因为自动机制响应变化的变化。随着这种传感器数据的复杂性和维度的增长，人类手动监控这些数据的能力越来越低。</p><h2id="研究时序变化的图结构的异常检测">研究时序变化的图结构的异常检测</h2><table><colgroup><col style="width: 9%" /><col style="width: 3%" /><col style="width: 4%" /><col style="width: 8%" /><col style="width: 8%" /><col style="width: 7%" /><col style="width: 28%" /><col style="width: 28%" /></colgroup><thead><tr class="header"><th style="text-align: center;">Dataset</th><th style="text-align: center;">#Node</th><th style="text-align: center;">#Edge</th><th style="text-align: center;">Edge Type</th><th style="text-align: center;">Max. Degree</th><th style="text-align: center;">Avg. Degree</th><th>#Timestamp</th><th style="text-align: center;">Paper</th></tr></thead><tbody><tr class="odd"><td style="text-align: center;">UCI Message<a href="#fn1"class="footnote-ref" id="fnref1"role="doc-noteref"><sup>1</sup></a></td><td style="text-align: center;">1899</td><td style="text-align: center;">13838</td><td style="text-align: center;">directed、unweighted、injectanomalies</td><td style="text-align: center;">255</td><td style="text-align: center;">14.57</td><td>190</td><td style="text-align: center;"><ahref="https://github.com/Ljiajie/Addgraph">AddGraph</a>、DynAD、Hierarchical、<ahref="https://github.com/LeiCaiwsu/StrGNN">StrGNN</a></td></tr><tr class="even"><td style="text-align: center;">Digg</td><td style="text-align: center;">30360</td><td style="text-align: center;">85155</td><td style="text-align: center;"></td><td style="text-align: center;">283</td><td style="text-align: center;">5.61</td><td>16</td><td style="text-align: center;">AddGraph、DynAD、StrGNN</td></tr><tr class="odd"><td style="text-align: center;">arXiv hep-th</td><td style="text-align: center;">6798</td><td style="text-align: center;">214693</td><td style="text-align: center;"></td><td style="text-align: center;">1590</td><td style="text-align: center;">63.16</td><td></td><td style="text-align: center;">DynAD</td></tr><tr class="even"><td style="text-align: center;">Enron<a href="#fn2" class="footnote-ref"id="fnref2" role="doc-noteref"><sup>2</sup></a></td><td style="text-align: center;">87036</td><td style="text-align: center;">530284</td><td style="text-align: center;">direcetd、unweighted、injectanomalies</td><td style="text-align: center;">1150</td><td style="text-align: center;">22</td><td></td><td style="text-align: center;">Hierarchical</td></tr><tr class="odd"><td style="text-align: center;">Facebook<a href="#fn3"class="footnote-ref" id="fnref3"role="doc-noteref"><sup>3</sup></a></td><td style="text-align: center;">60730</td><td style="text-align: center;">607487</td><td style="text-align: center;">undirected、unweighted、injectanomalies</td><td style="text-align: center;">203</td><td style="text-align: center;">9</td><td></td><td style="text-align: center;">Hierarchical</td></tr><tr class="even"><td style="text-align: center;">Math<a href="#fn4" class="footnote-ref"id="fnref4" role="doc-noteref"><sup>4</sup></a></td><td style="text-align: center;">24740</td><td style="text-align: center;">323357</td><td style="text-align: center;">directed、unweighted、injectanomalies</td><td style="text-align: center;">231</td><td style="text-align: center;">15</td><td></td><td style="text-align: center;">Hierarchical</td></tr><tr class="odd"><td style="text-align: center;">Email</td><td style="text-align: center;">2029</td><td style="text-align: center;">3724</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td>20</td><td style="text-align: center;">StrGNN</td></tr><tr class="even"><td style="text-align: center;">Topology</td><td style="text-align: center;">34761</td><td style="text-align: center;">107661</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td>21</td><td style="text-align: center;">StrGNN</td></tr><tr class="odd"><td style="text-align: center;">Bitcoin-alpha</td><td style="text-align: center;">3783</td><td style="text-align: center;">14124</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td>63</td><td style="text-align: center;">StrGNN</td></tr><tr class="even"><td style="text-align: center;">Bitcoin-otc</td><td style="text-align: center;">5881</td><td style="text-align: center;">21492</td><td style="text-align: center;"></td><td style="text-align: center;"></td><td style="text-align: center;"></td><td>63</td><td style="text-align: center;">StrGNN</td></tr></tbody></table><ul><li><strong>UCI Message</strong>：a directednetwork，其中包含了在加州大学欧文分校的一个在线社区中发布的信息，每个节点代表一个用户，每个有向边代表两个用户之间的一条消息</li><li><strong>Digg</strong>：社交新闻网站Digg旗下的一个响应网络。网络中的每个节点都是站点的用户，每条边都表示一个用户回复另一个用户。</li></ul><p><em>这两个数据集中的边都用时间戳进行了注释，在AddGraph<a href="#fn5"class="footnote-ref" id="fnref5"role="doc-noteref"><sup>5</sup></a>（使用）中，作者为每个节点随机生成一个初始向量作为其内容特征。</em></p><ul><li><strong>arXiv</strong>：来自e-print arXiv的collaborationnetwork，节点和边分别表示作者和协作链接。</li><li><strong>Enron</strong>：Enron公司的电子邮件数据集包含了大约50万封由安然公司的员工生成的电子邮件。美国联邦能源管理委员会在调查安然公司倒闭期间获得了这些电子邮件。</li><li><strong>Facebook</strong>：无向网络包含了Facebook用户的友谊数据。一个节点表示一个用户，一条边表示两个用户之间的友谊数据。该数据集并不完整，它包含了整个脸书友谊图中的一小部分子集。</li><li><strong>Math</strong>：这是一个在stack exchange网站MathOverflow上的时间交互网络，不同类型的边表示不同的交互关系。</li></ul><p><em>hierarchical GNN<a href="#fn6" class="footnote-ref" id="fnref6"role="doc-noteref"><sup>6</sup></a>使用伯努利分布注入不同比例的异常，然后将数据集分成两半分别作为训练集和测试集</em></p><ul><li><strong>Email</strong>：是民主党全国委员会的电子邮件。每个节点对应于一个人，边表示两个人之间的电子邮件交流。</li><li><strong>Topology</strong>：互联网自治系统之间的网络连接。节点是自治系统，边是自治系统之间的连接。</li><li><strong>Bitcoin-alpha和Bitcoin-otc</strong>：分别来自两个比特币平台Alpha和OTC，节点表示来自该平台的用户，如果一个用户对平台上的另一个用户进行评级，则在他们之间存在一条边。</li></ul><h2 id="dgraph数据集"><ahref="https://dgraph.xinye.com/introduction">DGraph数据集</a></h2><p><ahref="https://ai.ppdai.com/mirror/goToMirrorDetailSix?mirrorId=28&amp;tabindex=1">信也科技杯图算法大赛</a>、<ahref="https://github.com/DGraphXinye/DGraphFin_baseline">Github地址</a>、<ahref="https://openreview.net/forum?id=2rQPxsmjKF">DGraph论文openreview（含appendix）</a></p><p>大小是Elliptic的17倍，动态图上的节点异常检测，贴近工业背景</p><p>node attribute、node feature、edge weight、timestamp of the edge</p><figure class="highlight markdown"><table><tr><td class="code"><pre><span class="line">File dgraphfin.npz including below keys:  </span><br><span class="line"></span><br><span class="line">x: 17-dimensional node features.</span><br><span class="line"></span><br><span class="line">y: node label. There four classes. Below are the nodes counts of each class.     </span><br><span class="line">  0: 1210092    </span><br><span class="line">  1: 15509    </span><br><span class="line">  2: 1620851    </span><br><span class="line">  3: 854098    </span><br><span class="line">  Nodes of Class 1 are fraud users and nodes of 0 are normal users, and they the two classes to be predicted.    </span><br><span class="line">  Nodes of Class 2 and Class 3 are background users.    </span><br><span class="line"></span><br><span class="line">edge<span class="emphasis">_index: shape (4300999, 2). (directed)   </span></span><br><span class="line"><span class="emphasis">  Each edge is in the form (id_</span>a, id<span class="emphasis">_b), where ids are the indices in x.        </span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">edge_</span>type: 11 types of edges. </span><br><span class="line"></span><br><span class="line">edge<span class="emphasis">_timestamp: the desensitized timestamp of each edge.</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">train_</span>mask, valid<span class="emphasis">_mask, test_</span>mask:  </span><br><span class="line">  Nodes of Class 0 and Class 1 are randomly splitted by 70/15/15.    </span><br></pre></td></tr></table></figure><p>节点的出度表示信息完整度（可自控），入度表示被信任关系（不可自控）</p><p>统计：一个有label的节点他周围的节点是有label多的还是没有label的多</p><p>有向图-&gt;邻接矩阵，邻接链表-&gt;筛选出label=0，1的节点-&gt;</p><p>首先用dict得到每个节点的入度和出度个数</p><ol type="1"><li>有向图的节点出度数和入度数相等，先得到图中所有定点的平均度数：1.1622593938738837</li><li>正常节点有1210092个，其邻居中正常的有1277944个，欺诈的有9370个，bg2有1176560个，bg3有701840个。</li><li>异常节点有15509个，其邻居中正常的有9370个，欺诈的有424个，bg2有12657个，bg3有6527个。</li></ol><h3id="在gearsage的处理方法中与pyg的方法不同">在GearSage的处理方法中（与PyG的方法不同）：</h3><p>原来的npz文件：['x', 'y', 'edge_index', 'edge_type','edge_timestamp', 'train_mask', 'valid_mask', 'test_mask’]</p><p>处理后的data.pt：Data(x=[3700550, 31], edge_index=[2, 4300999],edge_attr=[4300999], y=[3700550, 1], train_mask=[857899],test_mask=[183840], valid_mask=[183862], edge_timestamp=[4300999])</p><h4id="对数据集的处理read_dgraphfinfolder">对数据集的处理：read_dgraphfin(folder)</h4><ol type="1"><li>增加节点入度——dim=1。将节点入度作为新加入的一列<imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/20230404093004.png" /></li><li>邻域时间戳信息——dim=2。构建edge_timestamp的邻接矩阵，增加edge_timestamp_sum（每个节点连边的时间戳的总和，然后normalize）和edge_timestamp_max（先增加反向边，然后求该节点相邻边的最大时间戳，并log）</li><li>边属性信息——dim=11。构建edge_attr的邻接矩阵，获取与当前节点相连的边中，不同类型的边的数量。</li></ol><h4 id="节点特征工程">节点特征工程</h4><ol type="1"><li>增加节点度数——dim=2。将节点的入度和出度作为新的特征</li><li>与邻域的相似度信息——dim=1。（将相邻节点的cosinesimilarity作为新的特征）</li><li>处理缺失值。将初始的17维节点特征中的缺失值-1替换为1</li><li>前景、背景节点信息——dim=2</li></ol><h4 id="边特征工程">边特征工程</h4><ol type="1"><li>加入反向边增加图稠密度——此时的图不一定是对称的，因为正向和反向边的时间戳不一定一样，但是type是一样的</li><li>构建边的方向性信息：</li><li>先把原来的edge_index的row和col各自concat，然后选择里面row小于col的边保留，</li></ol><p>tips：</p><p>1、<code>temp = scatter.scatter(a, row, dim=0, dim_size=y.size(0), reduce="sum")</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">torch_scatter.scatter(src: torch.Tensor, index: torch.Tensor, dim: <span class="built_in">int</span> = - <span class="number">1</span>, out: <span class="type">Optional</span>[torch.Tensor] = <span class="literal">None</span>, dim_size: <span class="type">Optional</span>[<span class="built_in">int</span>] = <span class="literal">None</span>, reduce: <span class="built_in">str</span> = <span class="string">&#x27;sum&#x27;</span>) → torch.Tensor</span><br></pre></td></tr></table></figure><p>根据index，将index相同值对应的src元素进行对应定义的计算，dim为在第几维进行相应的运算。e.g.scatter_sum即进行sum运算，scatter_mean即进行mean运算。</p><p><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/20230404111601.png" /></p><p>在gear的处理过程中，对于<code>a = F.one_hot(y[col])</code>，按照row，也就是边的src节点，把src节点相同的边的dst节点的onehot相加，输出的尺寸是[节点个数,2]，表示src所连dst节点的背景和前景节点情况。</p><p>接着，<code>temp += scatter.scatter(b, col, dim=0, dim_size=y.size(0), reduce="sum")</code>又将每个dst节点对应的src的onehot相加。</p><p>两个temp相加后，得到的是每个节点所连边（from+to）的节点背景节点的数量</p><p>增量学习和对邻域信息做衰减是什么？</p><aside id="footnotes" class="footnotes footnotes-end-of-document"role="doc-endnotes"><hr /><ol><li id="fn1"><p>1.http://konect.cc/networks/opsahl-ucsocial/<ahref="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li><liid="fn2"><p>2.http://networkrepository.com/ia-enron-email-dynamic.php<ahref="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn3"><p>3.http://networkrepository.com/fb-wosn-friends.php<ahref="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn4"><p>4.http://snap.stanford.edu/data/sx-mathoverflow.html<ahref="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li><li id="fn5"><p>5.https://www.ijcai.org/Proceedings/2019/0614.pdf<ahref="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li><liid="fn6"><p>6.https://ieeexplore.ieee.org/abstract/document/9377789/<ahref="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li></ol></aside>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AED </tag>
            
            <tag> dynamic graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>扩散模型</title>
      <link href="/posts/59918/"/>
      <url>/posts/59918/</url>
      
        <content type="html"><![CDATA[<p>一系列噪声扰动图中的边缘（也就是前向扩散过程），并通过学习将这个过程从噪声转换到数据来生成图(a.k.a,生成扩散过程)。由于排列不变性约束和同时考虑离散局部运动的必要性，有效地采用现有的方法来绘制数据是非常重要的Fs和图的整体拓扑性质。</p><p>这个github仓库整理了关于扩散模型的资源和论文集：https://github.com/heejkoo/Awesome-Diffusion-Models</p><p>这个是扩散模型综述的中文翻译：https://www.yuque.com/jinyuma-igdk2/hcuntc/hhlwis</p><p>扩散模型在图领域目前的应用还比较少</p><p>这篇文章通俗地解释了各个生成模型之间的关系https://zhuanlan.zhihu.com/p/591881660</p><p>想象平面上方的一个曲面，该曲面的高度对应于簇的密度。这个曲面映射出一个概率分布。使用这个概率分布来生成新的图像。所需要做的就是随机生成新的数据点，同时遵守更频繁地生成更可能的数据的限制——这一过程被称为“采样”分布。每个新的点都是一个新的图像。这一过程的挑战是学习构成训练数据的一些图像集的复杂概率分布。比如每张照片有一百万像素，绘制每张图像时需要一百万个轴。</p><p>非平衡热力学（Nonequilibriumthermodynamics）描述了扩散过程中每一步的概率分布。非常重要的是，每一步都是可逆的——只要有足够小的步骤，就可以从简单的分布（随机均匀的）回到复杂的分布。</p><p>在扩散过程中，在每个时间步长向每个像素添加一些噪声，将每个像素值向原点轻轻推一点。对数据集中的所有图像都这样做，那么百万维空间中的点的初始复杂分布（无法轻易描述和采样）就会变成原点周围点的简单正态分布。这一转换慢慢地将数据分布变成一个巨大的噪音球，这一正向过程提供了一个可以轻松采样的分布。</p><p>接下来使用神经网络，不断预测前一步噪声较小的图像，尽管他一开始会出错，我们可以不断调整网络参数。</p><p>与LLM结合的扩散模型能够根据文本生成图片，但由于大语言模型存在文化和社会偏见，由扩散模型生成的图像也会具有同样的问题。</p><h1 id="现有的生成模型">现有的生成模型</h1><p>估计数据<span class="math inline">\(\left \{x_i \right\}_i^n\)</span>的概率密度函数<span class="math inline">\(p(x)\)</span>，<span class="math inline">\(x_i \sim p(x)\)</span>。</p><p>要求概率密度函数：</p><ol type="1"><li><span class="math inline">\(p(x) \ge 0\)</span></li><li><span class="math inline">\(\int p(x)dx=1\)</span>这是一个很强的条件</li></ol><h2 id="基于能量的模型energy-based-model">基于能量的模型——Energy-basedModel</h2><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/diffusion/20230509102839.png" style="zoom:50%;" /></p><p>使用指数函数保证概率大于一，分母除以概率密度积分保证积分等于1，从而满足p(x)需要满足的性质</p><p><spanclass="math inline">\(E_\theta(x)\)</span>为物理学中的能量函数（郎之万Langevin动力学）</p><p>例如，softmax就是基于能量的分布。</p><p>有了p(x)的定义之后，要求参数<spanclass="math inline">\(\theta\)</span>，可以基于最大似然法训练，</p><p>使用马尔科夫链的蒙特卡罗方式，不断迭代，在高维的情况下具有很高的方差和噪声，导致训练方差高。</p><h2 id="自回归模型autoregressive-models">自回归模型——AutoregressiveModels</h2><p>将一个n维的概率密度函数变成n个一维的概率密度函数的乘积</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/diffusion/20230509104457.png" style="zoom:50%;" /></p><p>缺点：</p><ol type="1"><li>对于图片这种没有顺序的效果不好，生成的照片不真实，对于有序的音频效果较好</li><li>要采样n次，速度慢</li></ol><h2id="变分自编码器variational-autoencodersvae">变分自编码器——VariationalAutoencoders（VAE）</h2><p>从p(x)给定z的条件分布来进行采样，z使用简单的高斯分布。将从p(x)给定z建模为高斯分布。</p><p>理论上，根据montecarlo就可以在给定z的情况下估计出x，但为估计准确的p(x)，需要很多z，因为在大多数情况下</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/diffusion/20230509115846.png"/></p><h2 id="diffusion-model">Diffusion Model</h2><h3 id="ddpm">DDPM</h3><p>在VAE中x到latentspace的z再到x的过程中，p(z)、p(x|z)、q(z|x)都被建模成高斯分布，但是实际上，从z到x的过程是很复杂的，导致了VAE的表达能力不足。为解决这一问题，DM希望将z到x的过程分解成很多个简单的过程。</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/diffusion/20230509130204.png" style="zoom: 50%;" /></p><p>正向和反向的过程都被建模成高斯分布</p><p>反向用神经网络学习<spanclass="math inline">\(\mu\)</span>，最后要优化的函数同VAE</p><p>基于似然训练的，都比较稳定</p><p>（基于对抗训练的不太稳定）</p><p>最大的问题是采样速度慢</p><h3 id="sdesstochastic-differential-equations">SDEs（Stochasticdifferential equations）</h3><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/diffusion/20230509140823.png" style="zoom:50%;" /></p>]]></content>
      
      
      
        <tags>
            
            <tag> diffusion </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无监督异常检测</title>
      <link href="/posts/11901/"/>
      <url>/posts/11901/</url>
      
        <content type="html"><![CDATA[<h2 id="vae">VAE</h2><p><strong>方差</strong>是各个样本数据和<strong>平均数</strong>之差的平方和的平均数。</p><p><strong>标准差</strong>（StandardDeviation），又称均方差，是方差的平方根</p><p><strong>均方误差</strong>（Mean SquaredError）是各数据偏离<strong>真实值</strong>差值的平方和的平均数，均方误差的开方叫均方根误差，均方根误差才和标准差形式上接近。</p><p><strong>方差</strong>是数据序列与均值的关系，而<strong>均方误差</strong>是数据序列与真实值之间的关系</p><p>因此，AE的训练损失函数可以采用简单的MSE</p><p>训练AE并不需要对数据进行标注，所以<strong>AE是一种无监督学习方法</strong></p><p>压缩后得到的隐含特征也可以用来做一些其它工作，比如相似性搜索等</p><p>主成分分析（PCA）的主要目标是将特征维度变小，同时尽量减少信息损失。简而言之，对于一个样本矩阵：</p><ul><li>换特征，找一组新的特征来重新表示；</li><li>减少特征，新特征的数目要远小于原特征的数目。</li></ul><p>通过PCA将n维原始特征映射到维k（k&lt;n）上，称这k维特征为主成分。需要强调的是，不是简单地从n维特征中去除其余n-k维特征，而是重新构造出全新的k维正交特征，且新生成的k维数据尽可能多地包含原来n维数据的信息。例如，使用PCA将20个相关的特征转化为5个无关的新特征，并且尽可能保留原始数据集的信息。</p><h2id="生成式对抗网络gangenerative-adversarial-nets">生成式对抗网络GAN（GenerativeAdversarial Nets）</h2><p><ahref="https://baijiahao.baidu.com/s?id=1628492430897890432&amp;wfr=spider&amp;for=pc">万字综述之生成对抗网络</a></p><p>VAE与GAN的区别：</p><ol type="1"><li><p>GAN的思路比较粗暴，使用一个判别器去度量分布转换模块（即生成器）生成分布与真实数据分布的距离。</p></li><li><p>VAE 则没有那么直观，VAE 通过约束隐变量 z服从标准正态分布以及重构数据实现了分布转换映射 X=G(z)。</p></li></ol><p><a href="https://zhuanlan.zhihu.com/p/97482962">GAN做异常检测</a></p><p>使用GAN进行异常检测的任务是使用对抗性训练过程建模正常行为，并测量异常评分来检测异常</p><p>https://blog.csdn.net/cloudless_sky/article/details/123697481</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAD </tag>
            
            <tag> unsupervised </tag>
            
            <tag> VAE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>general heterophily</title>
      <link href="/posts/10605/"/>
      <url>/posts/10605/</url>
      
        <content type="html"><![CDATA[<h1 id="heterophily">heterophily</h1><p>heterophily与heterogeneity不同。Heterogeneity更多地与推荐系统中的节点类型差异有关，例如用户和项目节点，但heterophily是具有相同类型的节点下的邻居之间的特征或标签差异。传统的GNN通常假设相似的节点（特征/类）连接在一起，但“对立吸引”现象也广泛存在于一般图中。</p><p>这个github仓库收集了跟heterophily有关的论文：https://github.com/alexfanjn/Graph-Neural-Networks-With-Heterophily</p><h4id="iclr2023中了的heterophily的相关论文">ICLR2023中了的heterophily的相关论文</h4><p><a href="https://openreview.net/forum?id=JpRExTbl1-">Gradient Gatingfor Deep Multi-Rate Learning on Graphs</a></p><p>粒子动力学观点：<ahref="https://openreview.net/forum?id=4fZc_79Lrqs">ACMP: Allen-CahnMessage Passing with Attractive and Repulsive Forces for Graph NeuralNetworks, ICLR OpenReview</a></p><p>提供新的benchmark数据集：<ahref="https://openreview.net/forum?id=tJbbQfw-5wv">A critical look atthe evaluation of GNNs under heterophily: Are we really makingprogress?</a></p><p>动态图？：<a href="https://openreview.net/forum?id=8duT3mi_5n">GReTo:Remedying dynamic graph topology-task discordance via targethomophily</a></p><h4id="aaai2022合集httpsojs.aaai.orgindex.phpaaaiissuearchive">AAAI2022合集：https://ojs.aaai.org/index.php/AAAI/issue/archive</h4><h5id="aaai2022中heterophily的相关论文">AAAI2022中heterophily的相关论文</h5><p><ahref="https://ojs.aaai.org/index.php/AAAI/article/view/20319">BlockModeling-Guided Graph Convolutional Neural Networks</a>，<ahref="https://github.com/hedongxiao-tju/BM-GCN">code</a></p><p><a href="https://arxiv.org/abs/2112.13562">Powerful GraphConvolutioal Networks with Adaptive Propagation Mechanism for Homophilyand Heterophily</a>，<ahref="https://github.com/hedongxiao-tju/HOG-GCN">code</a></p><p><a href="https://arxiv.org/abs/2112.14438">Deformable GraphConvolutional Networks</a></p><p><a href="https://arxiv.org/abs/2110.00973">Graph Pointer NeuralNetworks</a></p><hr /><p>首先是2022的一篇综述：<font size=4><ahref="https://arxiv.org/abs/2202.07082">Graph Neural Networks for Graphswith Heterophily: A Survey</a></font></p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/taxonomy.png" alt="作者在文中使用的分类方法" style="zoom: 50%;" /></p><p>这个分类对应解决了两个问题：（1）如何发现合适的邻居；（2）如何利用所发现的邻居的信息。</p><h2 id="non-local-neighbor-extension">Non-local Neighbor Extension</h2><p>local neighborhood的定义在heterophilic图上是不恰当的，因为同一类的节点表现出很高的结构相似性，但彼此之间可能更远。两种方法可以从<strong>遥远</strong>但<strong>信息丰富</strong>的节点中捕获重要的特征，进而改进heterophilicGNNs的能力。</p><h3 id="high-order-neighbor-mixing">High-order neighbor mixing</h3><p>从节点的本地一跳邻居和<em>k</em>-hop跳邻居那里接收潜在的表示。代表性工作：</p><ol type="1"><li><font color="#FF0000">MixHop</font>：除了one-hop邻居外，还考虑了<strong>two-hopneighbors</strong>来进行消息传播。然后，从不同跳获得的消息通过不同的线性变换进行编码然后mixedby concatenation</li><li><font color="#FF0000">H2GCN</font>：在每个消息传递步骤中聚合来自<strong>higher-orderneighbors</strong>的信息。验证了当中心节点的one-hop邻居的标签有条件地独立于该节点的标签时，其two-hop邻居倾向于包含更多的与中心节点同一类的节点。</li></ol><h3 id="potential-neighbor-discovery">Potential neighbor discovery</h3><p>重新考虑异质图中的邻居定义，通过异质性下的整个拓扑探索构建新的structuralneighbors。其中，s(v, u)度量在特定定义的潜在空间中，节点u和v之间的距离。τ是控制邻居数量的阈值。</p><p><span class="math display">\[N_{p}(v)={u:s(v,u)&lt;τ}\]</span></p><ol type="1"><li><font color="#FF0000">Geom-GCN</font>：符合文中所定义的几何关系的节点，也参与了GCN的消息聚合。</li></ol><h2 id="gnn-architecture-refinement">GNN Architecture Refinement</h2><p>一般的GNN结构包含两部分： <span class="math display">\[\begin{split}\begin{aligned}m_{v}^{(l)} &amp;=AGGREGATE^{(L)} ({h_{u}^{(l-1)}:u\in N(v)}),\\h_{v}^{(l)} &amp;=UPDATE^{(L)}({h_{v}^{(l-1)},m}_{v}^{(l)})\end{aligned}\end{split}\]</span><strong>聚合函数</strong>整合已发现的邻居的信息，<strong>更新函数</strong>将学习到的邻居消息与初始的egorepresentation结合起来。</p><p>针对异质图上的原始局部邻居和扩展的非局部邻居，现有的GNN体系结构细化方法通过相应地修改聚合和更新函数来充分利用邻居信息。</p><h3 id="adaptive-message-aggregation">Adaptive Message Aggregation</h3><p>在异质图上整合有益信息的关键是区分相似邻居（可能在同一类中）的信息和不相似的邻居（可能属于不同的类别）的信息。</p><p>方法：在聚合操作中，对第l层的节点对(u, v)施加 adaptive edge-aware的权值<span class="math inline">\(a_{u,v}^{(l)}\)</span>来改变聚合操作<span class="math display">\[m_{v}^{(l)}=AGGREGATE^{(L)} (a_{u,v}^{(l)}{h_{u}^{(l-1)}:u\in N(v)}),\\\]</span>文中分别提供了现有方法在谱域和空间域采用的权值分配方案。其中，spectralGNNs使用图信号处理的理论来设计图的滤波器，而spatialGNNs则利用图的结构拓扑来开发聚合策略。</p><h4 id="spectral-domain">spectral domain</h4><p>与同质图上近似计算图傅里叶变换 (graph Fourier transformation)的拉普拉斯平滑 (Laplacian smoothing)和低通滤波 (low-passfifiltering)相比，异质图上的spectralGNNs包括低通和高通滤波器，以自适应地提取低频和高频图信号。其背后的本质直觉在于，低通滤波器主要保留了节点特征的共性(commonality)，而高通滤波器则捕获了节点之间的差异。</p><p><font color="#FF0000"> FAGCN</font>采用self-gating注意力机制，通过将<spanclass="math inline">\(a_{u,v}^{(l)}\)</span>分成两个分量，<em>i.e.</em>,<spanclass="math inline">\(a_{u,v}^{(l,LP)}\)</span> and <spanclass="math inline">\(a_{u,v}^{(l,HP)}\)</span>，来学习低频信号和高频信号的比例。通过自适应频率信号学习，FAGCN可以在不同类型的图上实现具有同质性和异质性的表达性能。</p><h4 id="spatial-domain">spatial domain</h4><p>在spatialdomain中，异质性gnn需要来自相同或不同类别的邻居的基于拓扑的不同聚合，而不是同质性gnn的averageaggregation。因此，应根据空间图的拓扑结构和节点标签来分配邻居的边感知权值(edge-awareweights)。</p><p><font color="#FF0000">DMP</font>以节点属性为弱标签，并考虑不同消息传递的节点属性异质性，并每条边指定每个属性传播权重。</p><h3 id="ego-neighbor-separation">Ego-neighbor separation</h3><p>在异质图上，一个ego node的类标签很可能与其相邻的节点不同。因此，将egonode representation与邻居节点的聚合表示分开encode将有利于可区分的nodeembedding learning。</p><p>ego-neighbor separationmethod在聚合过程中分开中心节点的自循环连接，同时把更新过程改为non-mixingoperations，例如concatenation，而不是混合操作，例如vanillaGCN中的average。</p><p><font color="#FF0000"> H2GCN</font>首先提出去掉自循环连接(self-loopconnection)，并且指出，更新函数中的非混合操作确保了表达性节点的表示能在多轮传播中存活下来，而不会变得非常相似。<font color="#FF0000"> WRGNN</font>对ego-nodeembedding及其邻居聚合消息使用不同的映射函数。</p><h3 id="inter-layer-combination">Inter-layer combination</h3><p>前面的两种方法深入研究了GNN的每一层，而Inter-layercombination考虑了层间操作来提高异质性下GNN的表示能力。</p><p>这个方法的直觉是，GNN的浅层(shallowlayers)收集局部信息，<em>e.g.</em>两层vanillaGCN中的一跳邻居位置，随着层的深入，GNN通过多轮邻居传播逐渐隐式地捕获全局信息。</p><p>在异质性设置下，具有相似信息的邻居，即类标签，可能同时在局部几何(localgeometry)和长期全局拓扑中(long-term globaltopology)定位。因此，结合来自每一层的中间表示有助于利用不同的邻域范围，并同时考虑局部和全局结构属性，从而产生强大的异质性GNNs。</p><p>这一想法最早来自于<font color="#FF0000">JKNet</font>，它首先在同质图上灵活地捕获不同邻域范围下更好的structure-awarerepresentation。由于异质图上的结构拓扑更加复杂。<font color="#FF0000">H2GCN</font>将之前所有层的节点表示连接起来，并在最后一层清晰地学习异质性节点特征。</p><h1 id="iclr2023上的相关论文">ICLR2023上的相关论文</h1><p><font size=4 > <a href="https://arxiv.org/abs/2211.14065">BeyondSmoothing: Unsupervised Graph Representation Learning with EdgeHeterophily Discriminating</a> </font></p><h2 id="motivation">Motivation</h2><p>已有的方法平滑相邻节点的表示，同时，为给表示学习提供监督信号，他们经常使用保持局部平滑性的目标，即鼓励在同一边内的节点表示、随机游走或子图具有更高的相似性。因此，所有的节点都被迫具有与它们的邻居相似的表示。如下图，所有连接的节点在表示空间中都被推得更近，即使其中一些节点与随机采样的节点相比只是具有适度的特征相似性。</p><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/similar representation.png" style="zoom:50%;" /></p><p><em>(Q1) Is it possible to distinguish between two types of edges inan unsupervised manner ?</em></p><p>在无监督的场景下，很难仅通过节点特征和图结构来区分边缘类型，特别是大量的边连接的是具有中等相似性的节点对。</p><p><em>(Q2) How to effectively couple edge discriminating withrepresentation learning into an integrated UGRL model?</em></p><p>作者想要找到一个良好的相互作用方案，使边识别和表示学习相互促进。</p><h2 id="methodology">Methodology</h2><p><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/framework.png" /></p><h3 id="edge-discriminating">Edge Discriminating</h3><p>用vectorial structural encoding(a.k.a. positionalencoding)对结构特征进行建模，通过将节点结构编码（位置编码）与原始特征连接起来，可以保留有效的证据。（本文使用的结构编码是Dwivediet al. 2022的基于随机游动扩散过程）</p><p>节点<span class="math inline">\(v_{i}\)</span>的<spanclass="math inline">\(d_{s}\)</span>维结构编码<spanclass="math inline">\(s_{i}\)</span>可以通过基于<spanclass="math inline">\(d_{s}\)</span>-step随机游走的graphdiffusion来计算</p><p><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/rw.png" /></p><p>其中<span class="math inline">\(T =AD^{-1}\)</span>是随机游走的转移矩阵。</p><blockquote><p>图增广方法有特征增广、结构增广、混合增广。</p><p>特征增广主要对图数据中的特征信息进行变换，最常见的手段是节点特征遮盖（NFM），即随机的将图中的一些特征量置为0；此外，节点特征乱序（NFS）也是一种特征增广方法，其手段为对调不同节点的特征向量。</p><p>结构增广的手段是对图结构信息进行变换，常见的结构增广为边修改（EM），包括对边的增加和删除；另一种结构增广为图弥散（Graphdiffusion，GD），其对不同阶的邻接矩阵进行加权求和，从而获取更全局的结构信息。</p><p>混合增广则结合了上述两种增广形式，一个典型的手段为子图采样（SS），即从原图数据中采样子结构成为增广样本。</p><p>——对graph diffusion的一个扩展</p></blockquote><p>以原始特征和结构编码为输入，边鉴别器用两个MLP层估计homophily的概率：</p><p><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/mlp.png" /></p><p>[·||·]表示连接操作，<spanclass="math inline">\(h_{i}^{&#39;}\)</span>是节点<spanclass="math inline">\(v_{i}\)</span>的中间嵌入，<spanclass="math inline">\(\theta_{i,j}\)</span>是<spanclass="math inline">\(e_{i,j}\)</span>的估计的homophily概率。为了使估计不受边方向的影响，我们将第二层MLP层应用于不同顺序的嵌入连接(embeddingconcatenations)。</p><p>利用估计的概率<spanclass="math inline">\(\theta_{i,j}\)</span>，作者想从伯努利分布<spanclass="math inline">\(w_{i,j}\simBernoulli(\theta_{i,j})\)</span>中采样一个二分类的homophily指标<spanclass="math inline">\(w_{i,j}\)</span>，<spanclass="math inline">\(w_{i,j}=0\)</span>代表homophily，<spanclass="math inline">\(w_{i,j}=1\)</span>代表heterophily。然而，这种采样是不可微的，会使得鉴别器难以训练。为解决这个问题，作者采用Gumbel-Max重参数化技巧来近似二进制指标<spanclass="math inline">\(w_{i,j}\)</span>。具体地说，离散的同质性指标<spanclass="math inline">\(w_{i,j}\)</span> is relaxed to一个连续的homophilyweight <span class="math inline">\(\hat{w}_{i,j}\)</span>：</p><p><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/sigmoid.png" /></p><p>其中，<span class="math inline">\(\delta \simUniform(0,1)\)</span>为抽样的Gumbel随机变量，<spanclass="math inline">\(\tau_{g}&gt;0\)</span>为 temperaturehyper-parameter。当<spanclass="math inline">\(\tau_{g}\)</span>接近0时，<spanclass="math inline">\(\hat{w}_{i,j}\)</span> tends to besharper（更接近0或1）。</p><p>为了使边鉴别可训练，作者没有明确地将边分为同质和异质两类，而是在后面homophilic/heterophilic这两个view上work的时候，为每个边分配一个软权重。</p><p><imgsrc="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/softweight.png" /></p><p>ranking loss：学习多个输入之间的关系。这种任务也叫metriclearning，即通过学习将原始输入投影到一个新的空间中，在这个空间里相似输入将表现出距离相近，而不同输入将表现出距离甚远。</p><h3 id="dual-channel-encoding">Dual-Channel Encoding</h3><p>为了从homophilic和heterophilicview中学习信息表示，作者设计了两个不同的编码器，它们分别在两个不同的vi上执行低通和高通graphfiltering。根据homophily边获取相似节点之间共享的信息，根据heterophily边从dissimilar的邻居中过滤出不相关的信息。</p><ol type="1"><li>具体地说，在homophilicview中，相似节点彼此连接，我们用低通图滤波器沿着homophilic结构平滑节点特征。低通滤波可以捕获图信号中的低频信息，从而保持相似节点的共享知识。GREET种使用的是一个简单的低通GNN，即SGC。</li><li>与homophilic view不同，heterophilicview包含连接不同节点的边，使用高通图滤波，沿边锐化节点特征并保留高频图信号。从而区分不同但连接的节点的表示。从图信号处理的角度来看，图拉普拉斯算子（即与图拉普拉斯矩阵L相乘）已被证明可以有效地捕获高频分量。因此使用Lap-SGC，来对heterophilicview进行高通滤波。</li></ol><p>使用两个不同的编码器（即SGC和Lap-SGC）获得两组分别捕获低频和高频信息的表示。最终节点表示通过连接由两个视图的表示得到。</p><h3 id="model-training">Model Training</h3><p>使用<strong>Pivot-Anchored RankingLoss</strong>训练边判别，使用<strong>contrastiveloss</strong>训练representationencoders，还引入了一种交替训练策略来迭代优化两个组件。</p><ol type="1"><li><strong>Pivot-Anchored RankingLoss</strong>：边识别的目标是区分homophilic（连接相似节点）和heterophilic（连接不同节点）边，其主要的挑战是找到“相似”和“不同”之间的界限。作者建议使用随机抽样的节点对作为相似度度量的“轴”(Pivot)，以确保由homophilic连接的节点对明显比“轴节点对”更相似，而由heterophilic连接的节点明显比“轴节点对”更不同。</li><li><strong>Robust Dual-Channel Contrastive Loss</strong></li><li><strong>Training Strategy</strong>：边识别模块的训练依赖于节点的表示来度量节点的相似性；表示学习反过来从边识别提供的视图生成表示。为了有效地训练两个组件，随着这两个组件的相互增强，作者采用交替训练策略来交替优化边识别和节点表示学习。总体优化目标可以写为<spanclass="math inline">\(L=L_{r}+L_{c}\)</span>。为了提高表示模块的泛化能力，作者采用数据增强来增加模型训练的数据多样性：featuremasking和edge dropping，以扰乱两个视图的特征和结构。</li></ol><h2 id="experiment">Experiment</h2><ol type="1"><li>作者也用adversarial attack进行攻击，证明模型的鲁棒性</li><li>首尾呼应，依旧可视化了Cora、扰动后的Cora和Texas的 pair-wiserepresentation similarity<ul><li>在(a)中，对于Cora数据集中的大多数边缘，末端节点的表示是相似的；值得注意的是，对于一小部分被识别为heterophilic的边，它们的相似性更接近于0。证明GREET可以分离homophilic and heterophilic edges，并产生可区分的表示。</li><li>(b)：在受扰动的Cora中，大量的边被检测为heterophilic，两端节点表示是不同的。由于GREET具有识别噪声边缘的能力，因此在对抗攻击面前显示出了很强的鲁棒性</li><li>(c)中也可以发现了类似的现象，其中大部分边被识别为heterophilic。分离的两种类型的边带来了更多的信息表示，从而导致了GREET在heterophilic图上的优越性能。</li></ul></li></ol><p><img src="https://testingcf.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/experiment.png" style="zoom:67%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAD </tag>
            
            <tag> heterophily </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
