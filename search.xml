<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>时序图异常检测</title>
      <link href="/posts/62968/"/>
      <url>/posts/62968/</url>
      
        <content type="html"><![CDATA[<p>随着车辆、工业系统和数据中心等网络物理系统（CPS）中的互联设备和传感器的快速增长，监测这些设备的需求越来越大以保护他们免受攻击。对于电网、水处理厂、交通和通信网络等关键基础设施尤其如此。</p><p>许多这样的现实世界系统涉及到大量的相互连接的传感器，这些传感器可以产生大量的时间序列数据。例如，在一个水处理厂中，可以有许多se测量其各部件中的水位、流量、水质、阀门状态等。<strong>来自这些传感器的数据可以以复杂的、非线性的方式联系起来</strong>：例如，openi一个阀门导致压力和流量的变化，导致进一步的变化，因为自动机制响应变化的变化。随着这种传感器数据的复杂性和维度的增长，人类手动监控这些数据的能力越来越低。</p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -GAD -time series -dynamic graph </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>扩散模型</title>
      <link href="/posts/59918/"/>
      <url>/posts/59918/</url>
      
        <content type="html"><![CDATA[<p>一系列噪声扰动图中的边缘（也就是前向扩散过程），并通过学习将这个过程从噪声转换到数据来生成图(a.k.a,生成扩散过程)。由于排列不变性约束和同时考虑离散局部运动的必要性，有效地采用现有的方法来绘制数据是非常重要的Fs和图的整体拓扑性质。</p><p>这个github仓库整理了关于扩散模型的资源和论文集：https://github.com/heejkoo/Awesome-Diffusion-Models</p><p>这个是扩散模型综述的中文翻译：https://www.yuque.com/jinyuma-igdk2/hcuntc/hhlwis</p><p>扩散模型在图领域目前的应用还比较少</p><p>这篇文章通俗地解释了各个生成模型之间的关系https://zhuanlan.zhihu.com/p/591881660</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>无监督异常检测</title>
      <link href="/posts/11901/"/>
      <url>/posts/11901/</url>
      
        <content type="html"><![CDATA[<h2 id="vae">VAE</h2><p><strong>方差</strong>是各个样本数据和<strong>平均数</strong>之差的平方和的平均数</p><p><strong>标准差</strong>（StandardDeviation），又称均方差，是方差的平方根</p><p><strong>均方误差</strong>（Mean SquaredError）是各数据偏离<strong>真实值</strong>差值的平方和的平均数，均方误差的开方叫均方根误差，均方根误差才和标准差形式上接近。</p><p><strong>方差</strong>是数据序列与均值的关系，而<strong>均方误差</strong>是数据序列与真实值之间的关系</p><p>因此，AE的训练损失函数可以采用简单的MSE</p><p>训练AE并不需要对数据进行标注，所以<strong>AE是一种无监督学习方法</strong></p><p>压缩后得到的隐含特征也可以用来做一些其它工作，比如相似性搜索等</p><p>主成分分析（PCA）的主要目标是将特征维度变小，同时尽量减少信息损失。简而言之，对于一个样本矩阵：</p><ul><li>换特征，找一组新的特征来重新表示；</li><li>减少特征，新特征的数目要远小于原特征的数目。</li></ul><p>通过PCA将n维原始特征映射到维k（k&lt;n）上，称这k维特征为主成分。需要强调的是，不是简单地从n维特征中去除其余n-k维特征，而是重新构造出全新的k维正交特征，且新生成的k维数据尽可能多地包含原来n维数据的信息。例如，使用PCA将20个相关的特征转化为5个无关的新特征，并且尽可能保留原始数据集的信息。</p><h2id="生成式对抗网络gangenerative-adversarial-nets">生成式对抗网络GAN（GenerativeAdversarial Nets）</h2><p><ahref="https://baijiahao.baidu.com/s?id=1628492430897890432&amp;wfr=spider&amp;for=pc">万字综述之生成对抗网络</a></p><p>VAE与GAN的区别：</p><ol type="1"><li><p>GAN的思路比较粗暴，使用一个判别器去度量分布转换模块（即生成器）生成分布与真实数据分布的距离。</p></li><li><p>VAE 则没有那么直观，VAE 通过约束隐变量 z服从标准正态分布以及重构数据实现了分布转换映射 X=G(z)。</p></li></ol><p><a href="https://zhuanlan.zhihu.com/p/97482962">GAN做异常检测</a></p><p>使用GAN进行异常检测的任务是使用对抗性训练过程建模正常行为，并测量异常评分来检测异常</p><p>https://blog.csdn.net/cloudless_sky/article/details/123697481</p>]]></content>
      
      
      <categories>
          
          <category> -文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -unsupervised -VAE -GAD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>general heterophily</title>
      <link href="/posts/10605/"/>
      <url>/posts/10605/</url>
      
        <content type="html"><![CDATA[<h1 id="heterophily">heterophily</h1><p>heterophily与heterogeneity不同。Heterogeneity更多地与推荐系统中的节点类型差异有关，例如用户和项目节点，但heterophily是具有相同类型的节点下的邻居之间的特征或标签差异。传统的GNN通常假设相似的节点（特征/类）连接在一起，但“对立吸引”现象也广泛存在于一般图中。</p><p>这个github仓库收集了跟heterophily有关的论文：https://github.com/alexfanjn/Graph-Neural-Networks-With-Heterophily</p><hr /><p>首先是2022的一篇综述：<font size=4><ahref="https://arxiv.org/abs/2202.07082">Graph Neural Networks for Graphswith Heterophily: A Survey</a></font></p><p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/taxonomy.png" alt="作者在文中使用的分类方法" style="zoom: 50%;" /></p><h2 id="non-local-neighbor-extension">Non-local Neighbor Extension</h2><p>local neighborhood的定义在heterophilic图上是不恰当的，因为同一类的节点表现出很高的结构相似性，但彼此之间可能更远。两种方法可以从<strong>遥远</strong>但<strong>信息丰富</strong>的节点中捕获重要的特征，进而改进heterophilicGNNs的能力。</p><h3 id="high-order-neighbor-mixing">High-order neighbor mixing</h3><p>从节点的本地一跳邻居和<em>k</em>-hop跳邻居那里接收潜在的表示。代表性工作：</p><ol type="1"><li><font color="#FF0000">MixHop</font>：除了one-hop邻居外，还考虑了<strong>two-hopneighbors</strong>来进行消息传播。然后，从不同跳获得的消息通过不同的线性变换进行编码然后mixedby concatenation</li><li><font color="#FF0000">H2GCN</font>：在每个消息传递步骤中聚合来自<strong>higher-orderneighbors</strong>的信息。验证了当中心节点的one-hop邻居的标签有条件地独立于该节点的标签时，其two-hop邻居倾向于包含更多的与中心节点同一类的节点。</li></ol><h3 id="potential-neighbor-discovery">Potential neighbor discovery</h3><p>重新考虑异质图中的邻居定义，通过异质性下的整个拓扑探索构建新的structuralneighbors。其中，s(v, u)度量在特定定义的潜在空间中，节点u和v之间的距离。τ是控制邻居数量的阈值。</p><p><span class="math display">\[N_{p}(v)={u:s(v,u)&lt;τ}\]</span></p><ol type="1"><li><font color="#FF0000">Geom-GCN</font>：符合文中所定义的几何关系的节点，也参与了GCN的消息聚合。</li></ol><h2 id="gnn-architecture-refinement">GNN Architecture Refinement</h2><p>一般的GNN结构包含两部分： <span class="math display">\[\begin{split}\begin{aligned}m_{v}^{(l)} &amp;=AGGREGATE^{(L)} ({h_{u}^{(l-1)}:u\in N(v)}),\\h_{v}^{(l)} &amp;=UPDATE^{(L)}({h_{v}^{(l-1)},m}_{v}^{(l)})\end{aligned}\end{split}\]</span><strong>聚合函数</strong>整合已发现的邻居的信息，<strong>更新函数</strong>将学习到的邻居消息与初始的egorepresentation结合起来。</p><p>针对异质图上的原始局部邻居和扩展的非局部邻居，现有的GNN体系结构细化方法通过相应地修改聚合和更新函数来充分利用邻居信息。</p><h3 id="adaptive-message-aggregation">Adaptive Message Aggregation</h3><p>在异质图上整合有益信息的关键是区分相似邻居（可能在同一类中）的信息和不相似的邻居（可能属于不同的类别）的信息。</p><p>方法：在聚合操作中，对第l层的节点对(u, v)施加 adaptive edge-aware的权值<span class="math inline">\(a_{u,v}^{(l)}\)</span>来改变聚合操作<span class="math display">\[m_{v}^{(l)}=AGGREGATE^{(L)} (a_{u,v}^{(l)}{h_{u}^{(l-1)}:u\in N(v)}),\\\]</span>文中分别提供了现有方法在谱域和空间域采用的权值分配方案。其中，spectralGNNs使用图信号处理的理论来设计图的滤波器，而spatialGNNs则利用图的结构拓扑来开发聚合策略。</p><h4 id="spectral-domain">spectral domain</h4><p>与同质图上近似计算图傅里叶变换 (graph Fourier transformation)的拉普拉斯平滑 (Laplacian smoothing)和低通滤波 (low-passfifiltering)相比，异质图上的spectralGNNs包括低通和高通滤波器，以自适应地提取低频和高频图信号。其背后的本质直觉在于，低通滤波器主要保留了节点特征的共性(commonality)，而高通滤波器则捕获了节点之间的差异。</p><p><font color="#FF0000"> FAGCN</font>采用self-gating注意力机制，通过将<spanclass="math inline">\(a_{u,v}^{(l)}\)</span>分成两个分量，<em>i.e.</em>,<spanclass="math inline">\(a_{u,v}^{(l,LP)}\)</span> and <spanclass="math inline">\(a_{u,v}^{(l,HP)}\)</span>，来学习低频信号和高频信号的比例。通过自适应频率信号学习，FAGCN可以在不同类型的图上实现具有同质性和异质性的表达性能。</p><h4 id="spatial-domain">spatial domain</h4><p>在spatialdomain中，异质性gnn需要来自相同或不同类别的邻居的基于拓扑的不同聚合，而不是同质性gnn的averageaggregation。因此，应根据空间图的拓扑结构和节点标签来分配邻居的边感知权值(edge-awareweights)。</p><p><font color="#FF0000">DMP</font>以节点属性为弱标签，并考虑不同消息传递的节点属性异质性，并每条边指定每个属性传播权重。</p><h3 id="ego-neighbor-separation">Ego-neighbor separation</h3><p>在异质图上，一个ego node的类标签很可能与其相邻的节点不同。因此，将egonode representation与邻居节点的聚合表示分开encode将有利于可区分的nodeembedding learning。</p><p>ego-neighbor separationmethod在聚合过程中分开中心节点的自循环连接，同时把更新过程改为non-mixingoperations，例如concatenation，而不是混合操作，例如vanillaGCN中的average。</p><p><font color="#FF0000"> H2GCN</font>首先提出去掉自循环连接(self-loopconnection)，并且指出，更新函数中的非混合操作确保了表达性节点的表示能在多轮传播中存活下来，而不会变得非常相似。<font color="#FF0000"> WRGNN</font>对ego-nodeembedding及其邻居聚合消息使用不同的映射函数。</p><h3 id="inter-layer-combination">Inter-layer combination</h3><p>前面的两种方法深入研究了GNN的每一层，而Inter-layercombination考虑了层间操作来提高异质性下GNN的表示能力。</p><p>这个方法的直觉是，GNN的浅层(shallowlayers)收集局部信息，<em>e.g.</em>两层vanillaGCN中的一跳邻居位置，随着层的深入，GNN通过多轮邻居传播逐渐隐式地捕获全局信息。</p><p>在异质性设置下，具有相似信息的邻居，即类标签，可能同时在局部几何(localgeometry)和长期全局拓扑中(long-term globaltopology)定位。因此，结合来自每一层的中间表示有助于利用不同的邻域范围，并同时考虑局部和全局结构属性，从而产生强大的异质性GNNs。</p><p>这一想法最早来自于<font color="#FF0000">JKNet</font>，它首先在同质图上灵活地捕获不同邻域范围下更好的structure-awarerepresentation。由于异质图上的结构拓扑更加复杂。<font color="#FF0000">H2GCN</font>将之前所有层的节点表示连接起来，并在最后一层清晰地学习异质性节点特征。</p><h1 id="iclr2023上的相关论文">ICLR2023上的相关论文</h1><p><font size=4 > <a href="https://arxiv.org/abs/2211.14065">BeyondSmoothing: Unsupervised Graph Representation Learning with EdgeHeterophily Discriminating</a> </font></p><h2 id="motivation">Motivation</h2><p>已有的方法平滑相邻节点的表示，同时，为给表示学习提供监督信号，他们经常使用保持局部平滑性的目标，即鼓励在同一边内的节点表示、随机游走或子图具有更高的相似性。因此，所有的节点都被迫具有与它们的邻居相似的表示。如下图，所有连接的节点在表示空间中都被推得更近，即使其中一些节点与随机采样的节点相比只是具有适度的特征相似性。</p><p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/similar representation.png" style="zoom:50%;" /></p><p><em>(Q1) Is it possible to distinguish between two types of edges inan unsupervised manner ?</em></p><p>在无监督的场景下，很难仅通过节点特征和图结构来区分边缘类型，特别是大量的边连接的是具有中等相似性的节点对。</p><p><em>(Q2) How to effectively couple edge discriminating withrepresentation learning into an integrated UGRL model?</em></p><p>作者想要找到一个良好的相互作用方案，使边识别和表示学习相互促进。</p><h2 id="methodology">Methodology</h2><p><imgsrc="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/framework.png" /></p><h3 id="edge-discriminating">Edge Discriminating</h3><p>用vectorial structural encoding(a.k.a. positionalencoding)对结构特征进行建模，通过将节点结构编码（位置编码）与原始特征连接起来，可以保留有效的证据。（本文使用的结构编码是Dwivediet al. 2022的基于随机游动扩散过程）</p><p>节点<span class="math inline">\(v_{i}\)</span>的<spanclass="math inline">\(d_{s}\)</span>维结构编码<spanclass="math inline">\(s_{i}\)</span>可以通过基于<spanclass="math inline">\(d_{s}\)</span>-step随机游走的graphdiffusion来计算</p><p><imgsrc="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/rw.png" /></p><p>其中<span class="math inline">\(T =AD^{-1}\)</span>是随机游走的转移矩阵。</p><blockquote><p>图增广方法有特征增广、结构增广、混合增广。</p><p>特征增广主要对图数据中的特征信息进行变换，最常见的手段是节点特征遮盖（NFM），即随机的将图中的一些特征量置为0；此外，节点特征乱序（NFS）也是一种特征增广方法，其手段为对调不同节点的特征向量。</p><p>结构增广的手段是对图结构信息进行变换，常见的结构增广为边修改（EM），包括对边的增加和删除；另一种结构增广为图弥散（Graphdiffusion，GD），其对不同阶的邻接矩阵进行加权求和，从而获取更全局的结构信息。</p><p>混合增广则结合了上述两种增广形式，一个典型的手段为子图采样（SS），即从原图数据中采样子结构成为增广样本。</p><p>——对graph diffusion的一个扩展</p></blockquote><p>以原始特征和结构编码为输入，边鉴别器用两个MLP层估计homophily的概率：</p><p><imgsrc="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/mlp.png" /></p><p>[·||·]表示连接操作，<spanclass="math inline">\(h_{i}^{&#39;}\)</span>是节点<spanclass="math inline">\(v_{i}\)</span>的中间嵌入，<spanclass="math inline">\(\theta_{i,j}\)</span>是<spanclass="math inline">\(e_{i,j}\)</span>的估计的homophily概率。为了使估计不受边方向的影响，我们将第二层MLP层应用于不同顺序的嵌入连接(embeddingconcatenations)。</p><p>利用估计的概率<spanclass="math inline">\(\theta_{i,j}\)</span>，作者想从伯努利分布<spanclass="math inline">\(w_{i,j}\simBernoulli(\theta_{i,j})\)</span>中采样一个二分类的homophily指标<spanclass="math inline">\(w_{i,j}\)</span>，<spanclass="math inline">\(w_{i,j}=0\)</span>代表homophily，<spanclass="math inline">\(w_{i,j}=1\)</span>代表heterophily。然而，这种采样是不可微的，会使得鉴别器难以训练。为解决这个问题，作者采用Gumbel-Max重参数化技巧来近似二进制指标<spanclass="math inline">\(w_{i,j}\)</span>。具体地说，离散的同质性指标<spanclass="math inline">\(w_{i,j}\)</span> is relaxed to一个连续的homophilyweight <span class="math inline">\(\hat{w}_{i,j}\)</span>：</p><p><imgsrc="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/sigmoid.png" /></p><p>其中，<span class="math inline">\(\delta \simUniform(0,1)\)</span>为抽样的Gumbel随机变量，<spanclass="math inline">\(\tau_{g}&gt;0\)</span>为 temperaturehyper-parameter。当<spanclass="math inline">\(\tau_{g}\)</span>接近0时，<spanclass="math inline">\(\hat{w}_{i,j}\)</span> tends to besharper（更接近0或1）。</p><p>为了使边鉴别可训练，作者没有明确地将边分为同质和异质两类，而是在后面homophilic/heterophilic这两个view上work的时候，为每个边分配一个软权重。</p><p><imgsrc="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/softweight.png" /></p><p>ranking loss：学习多个输入之间的关系。这种任务也叫metriclearning，即通过学习将原始输入投影到一个新的空间中，在这个空间里相似输入将表现出距离相近，而不同输入将表现出距离甚远。</p><h3 id="dual-channel-encoding">Dual-Channel Encoding</h3><p>为了从homophilic和heterophilicview中学习信息表示，作者设计了两个不同的编码器，它们分别在两个不同的vi上执行低通和高通graphfiltering。根据homophily边获取相似节点之间共享的信息，根据heterophily边从dissimilar的邻居中过滤出不相关的信息。</p><ol type="1"><li>具体地说，在homophilicview中，相似节点彼此连接，我们用低通图滤波器沿着homophilic结构平滑节点特征。低通滤波可以捕获图信号中的低频信息，从而保持相似节点的共享知识。GREET种使用的是一个简单的低通GNN，即SGC。</li><li>与homophilic view不同，heterophilicview包含连接不同节点的边，使用高通图滤波，沿边锐化节点特征并保留高频图信号。从而区分不同但连接的节点的表示。从图信号处理的角度来看，图拉普拉斯算子（即与图拉普拉斯矩阵L相乘）已被证明可以有效地捕获高频分量。因此使用Lap-SGC，来对heterophilicview进行高通滤波。</li></ol><p>使用两个不同的编码器（即SGC和Lap-SGC）获得两组分别捕获低频和高频信息的表示。最终节点表示通过连接由两个视图的表示得到。</p><h3 id="model-training">Model Training</h3><p>使用<strong>Pivot-Anchored RankingLoss</strong>训练边判别，使用<strong>contrastiveloss</strong>训练representationencoders，还引入了一种交替训练策略来迭代优化两个组件。</p><ol type="1"><li><strong>Pivot-Anchored RankingLoss</strong>：边识别的目标是区分homophilic（连接相似节点）和heterophilic（连接不同节点）边，其主要的挑战是找到“相似”和“不同”之间的界限。作者建议使用随机抽样的节点对作为相似度度量的“轴”(Pivot)，以确保由homophilic连接的节点对明显比“轴节点对”更相似，而由heterophilic连接的节点明显比“轴节点对”更不同。</li><li><strong>Robust Dual-Channel Contrastive Loss</strong>：</li><li><strong>Training Strategy</strong>：边识别模块的训练依赖于节点的表示来度量节点的相似性；表示学习反过来从边识别提供的视图生成表示。为了有效地训练两个组件，随着这两个组件的相互增强，作者采用交替训练策略来交替优化边识别和节点表示学习。总体优化目标可以写为<spanclass="math inline">\(L=L_{r}+L_{c}\)</span>。为了提高表示模块的泛化能力，作者采用数据增强来增加模型训练的数据多样性：featuremasking和edge dropping，以扰乱两个视图的特征和结构。</li></ol><h2 id="experiment">Experiment</h2><ol type="1"><li>作者也用adversarial attack进行攻击，证明模型的鲁棒性</li><li>首尾呼应，依旧可视化了Cora、扰动后的Cora和Texas的 pair-wiserepresentation similarity<ul><li>在(a)中，对于Cora数据集中的大多数边缘，末端节点的表示是相似的；值得注意的是，对于一小部分被识别为heterophilic的边，它们的相似性更接近于0。证明GREET可以分离homophilic and heterophilic edges，并产生可区分的表示。</li><li>(b)：在受扰动的Cora中，大量的边被检测为heterophilic，两端节点表示是不同的。由于GREET具有识别噪声边缘的能力，因此在对抗攻击面前显示出了很强的鲁棒性</li><li>(c)中也可以发现了类似的现象，其中大部分边被识别为heterophilic。分离的两种类型的边带来了更多的信息表示，从而导致了GREET在heterophilic图上的优越性能。</li></ul></li></ol><p><img src="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/hetero/experiment.png" style="zoom:67%;" /></p>]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAD </tag>
            
            <tag> heterophily </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>建站备忘录</title>
      <link href="/posts/56120/"/>
      <url>/posts/56120/</url>
      
        <content type="html"><![CDATA[<p>防止记性不好忘记了或者换电脑找不到啦~~</p><h3 id="网站">网站</h3><p>图标库：https://fontawesome.com/icons</p><h3 id="hexo命令">hexo命令</h3><p>新建一篇文章：<code>hexo new post "article title"</code></p><p>新建页面：<code>hexo new page "page name"</code></p><p>Hexo 默认的静态 <code>url</code>格式是：<code>:year/:month/:day/:title</code>，是按照年、月、日、文章标题来生成固定链接的。如：<code>http://id.github.io/2022/11/23/hello-world</code>。</p><p>使用 Abbrlink插件可以使每篇文章都有一个唯一的编号，并将文章的链接用这个编号唯一区别，这样链接中不会出现中文，也不会因为修改文章的日期而导致链接的改变。</p><p>使用gulp压缩静态资源</p><p>【码一下】<span style="background-color: #EDFF3F;">组会完了研究一下<ahref="https://github.com/MoePlayer/hexo-tag-aplayer">音乐</a>和评论</span></p><h3 id="markdown">Markdown</h3><ul><li>直接在markdown中使用和html语法改变文字大小、颜色和背景：</li></ul><p><font size=4 > 这里输入文字，自定义字体大小 </font><font color="#FF0000"> 这里输入文字，自定义字体颜色</font> <spanstyle="background-color: #ff6600;">这里输入文字，自定义字体背景色</span><font color="#000000" size=4><spanstyle="background-color: #ADFF2F;">这是综合起来的效果 </span></font><font color="#FFFFFF" size=4><spanstyle="background-color: #68228B;">这是综合起来的效果2</span></font></p><ul><li><p>正常显示md中的latex公式</p><ul><li><p><code>npm un hexo-math</code><code>npm un hexo-renderer-marked</code></p></li><li><p>安装hexo-renderer-pandoc渲染器：<code>npm i hexo-renderer-pandoc</code></p></li><li><p>官网下载<ahref="https://pandoc.org/installing.html">pandoc</a></p></li><li><p>主题配置下的mathjax设置：</p><figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">mathjax:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span></span><br><span class="line">  <span class="attr">per_page:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li><li><p>（在有latex公式的文档前面加<code>mathjax: true</code>）</p></li><li><p>配置到这儿就可以正常显示了，后面还有问题的可以参考这篇文章：https://blog.csdn.net/qq_52466006/article/details/126924064</p></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> 建站 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAD </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CARE-GNN</title>
      <link href="/posts/45529/"/>
      <url>/posts/45529/</url>
      
        <content type="html"><![CDATA[<h2id="enhancing-graph-neural-network-based-fraud-detectors-against-camouflaged-fraudsters">EnhancingGraph Neural Network-based Fraud Detectors against CamouflagedFraudsters</h2><blockquote><p>Yingtong Dou1, Zhiwei Liu1, Li Sun2, Yutong Deng2, Hao Peng3, PhilipS. Yu1</p></blockquote><p>会议：CIKM '20</p><p>原文地址：https://dl.acm.org/doi/abs/10.1145/3340531.3411903</p><p>参考翻译：https://blog.csdn.net/jingcao233/article/details/121718108</p><h2 id="introduction">introduction</h2><hr /><p>Graph-based methods can reveal the suspiciousness of these entitiesat the graph level, since fraudsters with the same goal tend to connectwith each otherTextual features</p><p>GNN-based methods aggregate聚合 neighborhood information to learn therepresentation of a center node with neural modules. They can be trainedin an end-to-end and semi-supervised fashion, which saves much featureengineering and data annotation cost.</p><h2 id="motivation">motivation：</h2><ul><li>ignoring the camouflage behaviors of fraudsters</li><li>the limitations and vulnerabilities of GNNs when graphs have noisynodes and edges</li><li>Though some recent works have noticed similar challenges, eitherfail to fit the fraud detection problems or break the end-to-endlearning fashion of GNNs</li><li>Directly applying GNNs to graphs with camouflaged fraudsters willhamper the neighbor aggregation process of GNNs.</li></ul><figure><imgsrc="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/1.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure><h2 id="two-types-of-camouflages">two types of camouflages:</h2><ul><li>Feature camouflage: fraudsters may ==adjust their behaviors==, addspecial characters in reviews, or employ deep language generation modelsto gloss over explicit suspicious outcomes</li><li>Relation camouflage: fraudsters camouflage themselves via‘connecting to many benign entities’</li></ul><h2 id="method">method:</h2><ol type="1"><li>For the feature camouflage: label-aware similarity measure to findthe most similar neighbors based on node features. Specifically, wedesign a neural classifier as a similarity measure, which is directlyoptimized according to experts with domain knowledge (i.e., annotateddata).基于标签感知的相似性度量计算其邻居相似性</li><li>For the relation camouflage: devise a similarity-aware neighborselector to select the similar neighbors of a center node within arelation. Furthermore, we leverage reinforcement learning (RL) toadaptively find the optimal neighbor selection threshold along with theGNN training process.</li><li>We utilize the neighbor filtering thresholds learned by RL toformulate a relation-aware neighbor aggregator which combinesneighborhood information from different relations and obtains the finalcenter node representation获得中心节点表示</li></ol><h2 id="advantages-benefits">advantages &amp; benefits:</h2><ul><li>Adaptability. CARE-GNN adaptively selects best neighbors foraggregation given arbitrary multi-relation graph.</li><li>High-efficiency. CARE-GNN has a high computational efficiencywithout attention and deep reinforcement learning.</li><li>Flexibility. Many other neural modules and external knowledge can beplugged into the CARE-GNN</li></ul><h2 id="steps">steps：</h2><figure><imgsrc="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/2.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure><ol type="1"><li>construct a multi-relation graph based on domain knowledge.</li><li>first compute its neighbor similarities based with proposedlabel-aware similaritymeasure.基于标签感知的相似性度量计算其邻居相似性，通过对比节点特征来寻找最相似的邻居。<ol type="1"><li>high time complexity×</li><li>employ a one-layer MLP as the node label predictor at each layer anduse the l1-distance between the prediction results of two nodes as theirsimilaritymeasure.在每一层采用一层MLP作为节点标签预测器，并使用两个节点预测结果之间的距离作为它们的相似性度量<ol type="1"><li>Optimization. To train the similarity measure together with GNNs, aheuristic approach is to append it as a new layer before the aggregationlayer of GCN [ 20 ]. However, if the similarity measure could noteffectively filter the camouflaged neighbors at the first layer, it willhamper the performance of following GNN layers. Consequently, the MLPparameters cannot be well-updated through the back-propagation process.To train the similar measure with a direct supervised signal fromlabels, like [ 35], we define the cross-entropy loss of the MLP atl-layer asv the similarity measure parameters are directly updatedthrough the above lossfunction.为了与gnn一起训练相似性度量，一种启发式的方法是在GCN[20]的聚合层之前添加一个新的层。但是，如果相似性度量不能有效地过滤第一层的伪装邻居，则会影响后续GNN层的性能。因此，MLP参数不能通过反向传播过程很好地更新。为了用来自标签的直接监督信号(如[35])训练相似度量，我们定义了l层MLP的交叉熵损失，因为相似度量参数直接通过上述损失函数更新。</li></ol></li></ol></li><li>Similarity-aware Neighbor Selector：Then filter the dissimilarneighbors under each relation with the proposed neighborselector.相似度感知的邻居选择器：使用邻居选择器过滤每个关系下的不同邻居：选择一个中心节点的某个关系下的相似邻居。另外，在训练过程中，使用强化学习（RL）自适应地找到最佳邻居选择阈值<ol type="1"><li>It selects similar neighbors under each relation using top-psampling with an adaptive filteringthreshold.它使用具有自适应滤波阈值的top-p采样在每个关系下选择相似的邻居。</li><li>formulate the RL process as B(A,f,T).A是action space，f是rewardfunction，T是terminal condation. an initial pr(l)初始阈值<ol type="1"><li>action: The action represents how RL updates the pr(l) based on thereward</li><li>reward: The optimal pr(l) is expected to find the most similar(i.e.,minimum distances in Eq.</li><li>define the reward for epoch e as：epoch e的奖励定义为：<ol type="1"><li><p>当epoch eee上新选择的邻居的平均距离小于前一个epoch时，奖励为正，反之为负。但估计累加奖励并不容易，因此设计了无需搜索的贪婪策略，使用即时奖励来更新动作。</p><figure><imgsrc="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/3.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure></li></ol></li><li>终止条件：<ol type="1"><li>RL在最近的10个epoch收敛，并发消息最优阈值。RL模块终止后，过滤阈值固定为最优阈值，直到GNN收敛。</li></ol><figure><imgsrc="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/4.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure></li></ol></li></ol></li><li>Then, the GNN is trained with partially labeled nodes supervised bybinary classification loss functions. The neighbor selector is optimizedusing reinforcement learning during training theGNN.用二元分类损失函数，使用部分标记节点进行监督训练。训练期间使用强化学习优化邻居选择器：构造一个关系感知的邻居聚合器，该聚合器将来自不同关系的邻居信息结合起来，得到最终的中心节点表示。</li><li>Instead of directly aggregating the neighbors for all relations, weseparate the aggregation part as intra-relation aggregation关系内聚合and inter-relation aggregation关系间聚合 process. During theintra-relation aggregation process, the embedding of neighbors undereach relation is aggregated simultaneously. Then, the embeddings foreach relation are combined during the inter-relation aggregationprocess.<ol type="1"><li><p>intra-relation neighbor aggregation:</p><figure><imgsrc="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/5.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure></li><li><p>inter-relation aggregation:</p><figure><imgsrc="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/6.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure></li></ol></li><li>Finally, the node embeddings at the last layer are used forprediction.</li></ol><figure><imgsrc="https://gcore.jsdelivr.net/gh/fortunato-all/tuchuang/论文阅读笔记/7.png"alt="Untitled" /><figcaption aria-hidden="true">Untitled</figcaption></figure><p>做出了两个数据集</p><h2 id="存在问题">存在问题：</h2><p>mlp预测不准的话，会删掉很重要的或者良性的边。</p><p>两种相差很大的benign，互相对对方的学习有帮助</p><p>直接删掉会对模型有很大问题</p><h1id="reinforced-neighborhood-selection-guided-multi-relational-graph-neural-networks">ReinforcedNeighborhood Selection Guided Multi-Relational Graph NeuralNetworks</h1><hr /><blockquote><p>HAO PENG∗, Beihang University, China<br>RUITONG ZHANG, BeihangUniversity, China<br>YINGTONG DOU, University of Illinois at Chicago,USA<br>RENYU YANG, University of Leeds, UK<br>JINGYI ZHANG, BeihangUniversity, China<br>PHILIP S. YU, University of Illinois at Chicago,USA</p></blockquote><h1id="alleviating-the-inconsistency-problem-of-applying-graph-neural-network-to-fraud-detection"><strong>Alleviatingthe Inconsistency Problem of Applying Graph Neural Network to FraudDetection</strong></h1><hr /><blockquote><p>Zhiwei Liu, Yingtong Dou, Yutong Deng, Hao Peng</p></blockquote><p>the inconsistency problem incurred by fraudsters is hardlyinvestigated</p><p><a href="https://www.cnblogs.com/C-W-K/p/13621603.html">《Alleviatingthe Inconsistency Problem of Applying Graph Neural Network to FraudDetection》阅读笔记</a></p><p><ahref="https://blog.csdn.net/ypp0229/article/details/108441100">论文阅读笔记：Graphconsis---Alleviatingthe Inconsistency Problem of Applying Graph Neural Network toFraud_麦地与诗人的博客-CSDN博客</a></p><h1id="pick-and-choose-a-gnn-based-imbalanced-learning-approach-for-fraud-detection">Pickand Choose: A GNN-based Imbalanced Learning Approach for FraudDetection</h1><hr /><p><ahref="https://zhuanlan.zhihu.com/p/536942199">中科院敖翔：如何用图神经网络应对互联网金融欺诈？</a></p><p>在sample子图的时候尽可能地选minority点，在构造邻居的时候尽可能增大minority的邻居，减少majority的邻居</p><p>他们专注于修改邻接矩阵来修剪噪声边缘或保持一个平衡的邻域标签频率，</p><h1id="rethinking-graph-neural-networks-for-anomaly-detection">RethinkingGraph Neural Networks for Anomaly Detection</h1><hr /><p><a href="https://cloud.tencent.com/developer/article/2021370">ICML2022 | 基于结构化数据的异常检测再思考:我们究竟需要怎样的图神经网络？</a></p><p><ahref="https://www.pudn.com/news/62d94169864d5c73acdb29d3.html">【GNN报告】香港科技大学李佳：图异常检测再思考-我们究竟需要怎样的图神经网络？-pudn.com</a></p><p><ahref="https://blog.csdn.net/c9Yv2cf9I06K2A9E/article/details/125229976"></a></p><h1id="fraudre-fraud-detection-dual-resistant-to-graph-inconsistency-and-imbalance">FRAUDRE:Fraud Detection Dual-Resistant to Graph Inconsistency and Imbalance</h1><hr />]]></content>
      
      
      <categories>
          
          <category> 文献阅读 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> GAD </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
